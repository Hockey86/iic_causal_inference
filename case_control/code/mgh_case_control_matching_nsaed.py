# -*- coding: utf-8 -*-
"""MGH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1328EbQmeZ9maTWn5AGkikTTttcHecddR

# Setting up
"""

import sys
import os
import time
#!{sys.executable} -m pip install keras-rl

import numpy as np
import pickle
import pandas as pd
import scipy
from fastdtw import fastdtw
from scipy.signal import butter, lfilter, freqz

import neuralnet 

import torch
import torch.nn as nn

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import matplotlib.transforms as transforms
import seaborn as sns

import sklearn.linear_model as linearmodel
import sklearn.ensemble as ensemble
import sklearn.svm as svm
from bartpy.sklearnmodel import SklearnModel as bart
from sklearn.model_selection import cross_validate
from sklearn.cluster import DBSCAN
from sklearn.svm import SVR

import scipy.optimize as opt

import warnings
warnings.filterwarnings("ignore")

#import gym
#from gym import spaces
#from gym.utils import seeding
#
#from keras.models import Sequential
#from keras.layers import Dense, Activation, Flatten
#from keras.optimizers import Adam
#
#from rl.agents.dqn import DQNAgent
#from rl.policy import EpsGreedyQPolicy
#from rl.memory import SequentialMemory

#import pickle
#
#from sklearn.ensemble import RandomForestRegressor as RFR
#from sklearn.svm import SVR
#
#import sklearn.linear_model as lm
#
#import sklearn.neural_network as nn

#import torch
#import torch.nn as nn
window = 600
datafile = open('../IICdata/preprocessed_dataset_all.pickle','rb')
data = pickle.load( datafile , encoding='latin')

big_datafile = open('../IICdata/for_Harsh/data/big_dataset.pickle','rb')
big_dataset = pickle.load( big_datafile , encoding='latin')

featurefile = open('../IICdata/for_Harsh/data/features_window_%d.pickle'%(window),'rb')
features = pickle.load( featurefile , encoding='latin')



"""# Q-Learning
Defining the model
"""
def butter_lowpass(cutoff, fs, order=5):
    nyq = 0.5 * fs
    normal_cutoff = cutoff / nyq
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    return b, a

def butter_lowpass_filter(data, cutoff, fs, order=5):
    b, a = butter_lowpass(cutoff, fs, order=order)
    y = lfilter(b, a, data)
    return y


class QLearning:
    def __init__( self, state_space_dimension, action_space_dimension, covariate_space_dimension, reward = ( lambda x: 0 ), discount=1, off_policy=True, model='ridge' ):
        self.state_space_dimension = state_space_dimension
        self.action_space_dimension = action_space_dimension
        self.covariate_space_dimension = covariate_space_dimension
        self.model = model
        if self.model=='bart':
            self.Q = bart()
            self.A = lambda s1,c: opt.minimize(fun = lambda a1: -1*self.Q.predict([np.hstack((s1,a1,c))])[0], x0 = np.zeros((self.action_space_dimension,)), method='CG').x
            self.S = bart()
        if self.model=='ridge':
            self.Q = linearmodel.Ridge()
            self.A = lambda s1,c: opt.minimize(fun = lambda a1: -1*self.Q.predict([np.hstack((s1,a1,c))])[0], x0 = np.zeros((self.action_space_dimension,)), method='CG').x
            self.S = linearmodel.Ridge()
        if self.model=='random-forest':
            self.Q = ensemble.RandomForestRegressor()
            self.A = lambda s1,c: opt.minimize(fun = lambda a1: -1*self.Q.predict([np.hstack((s1,a1,c))])[0], x0 = np.zeros((self.action_space_dimension,)), method='CG').x
            self.S = ensemble.RandomForestRegressor()
        if self.model=='neural-net':
            self.Q = neuralnet.Net(self.state_space_dimension+self.action_space_dimension+self.covariate_space_dimension,1,[16],isOutputPositive=0)
            self.A = neuralnet.Net(self.state_space_dimension+self.action_space_dimension+self.covariate_space_dimension,self.action_space_dimension,[48],isOutputPositive=1)
            self.S = neuralnet.Net(self.state_space_dimension+self.action_space_dimension+self.covariate_space_dimension,self.state_space_dimension,[32],isOutputPositive=1)
        self.off_policy = off_policy
        self.discount = discount
        self.reward = reward
        self.lag = 1
        self.forecast = 1
    
    def fit( self, features, learningrate=0.05, losscutoff=1001296, verbose=False, itr=5):
        Y = features['Y']
        C = features['C']
        E = features['E']
        D = features['D']
        Y_val = features['Y_val']
        N = len(Y)
        if self.model=='ridge' or self.model=='random-forest':
            #first fit the last step
            XE = np.array([ E[i][-1] for i in range(0,N) ])
            XD = np.array([ D[i][-1] for i in range(0,N) ])
            X = np.hstack((XE,XD,C))
            self.Q = self.Q.fit(X,Y_val)
            
            #now learn other steps
            for itrtn in range(0,itr):
                for i in range(0,N):
                    T,ps = E[i].shape
                    if T>2:
                        XCi = np.array([ C[i] for t in range(0,T) ])
                        Xi = np.hstack((E[i],D[i],XCi))
                        XE1i = E[i][1:,:]
                        if self.off_policy:
                            A1i = np.array([ self.A( E[i][t+1],C[i] ) for t in range(0,T-1) ])
                        else:
                            A1i = D[i][1:,:]
                        #print((np.shape(XE1i),np.shape(A1i),np.shape(XCi[:T-1,:])) )
                        X1i = np.hstack((XE1i,A1i,XCi[:T-1,:]))
                        Y_vali = np.hstack( ( self.discount*self.Q.predict(X1i) + np.array([self.reward(E[i][t],D[i][t]) for t in range(0,T-1)]), np.array([ Y_val[i] ]) ) )
                        if i==0:
                            X = Xi
                            Y_val = Y_vali
                        else:
                            X = np.vstack((X,Xi))
                            Y_val = np.hstack((Y_val,Y_vali))
                self.Q = self.Q.fit(X,Y_val)
        return self
                
    def fit_simulator( self, features, lag=1, forecast=5, learningrate=0.05, losscutoff=1001296, verbose=False):
        Y = features['Y']
        C = features['C']
        E = features['E']
        D = features['D']
        N = len(Y)
        self.lag = lag
        self.forecast = forecast
        if self.model=='ridge' or self.model=='random-forest':
            X = []
            Out = []
            for itrtn in range(0,1):
                for i in range(0,N): #for a particular patient:
                    T,ps = E[i].shape #time they are in observation
                    if T>lag+1: #checking if they are long enough there
                        for t in range(0,T-(lag+forecast)+1):
                            ex, dx, cx = E[i][t:t+lag,:].reshape(1,-1)[0], D[i][t:t+lag,:].reshape(1,-1)[0], C[i]
                            Xit = np.hstack((ex,dx,cx)) 
                            Yit = E[i][t+lag:t+lag+forecast,:].reshape(1,-1)[0]
                            X.append(Xit)
                            Out.append(Yit)
            X = np.array(X)
            sample_weights = 10000*(np.sum(X[:,2*lag:4*lag],axis=1)) + 1
            scores = cross_validate(self.S,np.array(X),np.array(Out),cv=5)
            print(np.mean(scores['test_score']))
            self.S = self.S.fit(np.array(X),np.array(Out),sample_weight=sample_weights)  
            print(self.S.score(np.array(X),np.array(Out)))                  
        return self
    
    def run_simulator(self, init_features,D_forecast=[],T=20):
        C = init_features['C'] #C, only for one person, length = 1
        E = init_features['E'] #E, only for one person, length = lag
        D = init_features['D'] #D, only for one person, length = lag-1    
        lag = self.lag
        for t in range(0,T):
            if len(D_forecast)<T:
                D1 = self.A(E[-1,:],C) #predict this step action
            else:
                D1 = D_forecast[t]
            if len(D)>0:
                D = np.vstack((D,D1))
            else:
                D = D1.reshape(1,-1)
            ex, dx, cx = E[-lag:,:].reshape(1,-1)[0], D[-lag:,:].reshape(1,-1)[0], C
            Xit = np.hstack((ex,dx,cx)) 
            E1 = self.S.predict(Xit.reshape(1,-1)) #predicting next K steps
            E1 = E1.reshape((self.forecast,2))
            E = np.vstack((E,E1[0,:].reshape(1,-1)))
        return E,D
            
    
def f(y):
    if y==0 or y==1 or y==2:
        return 0
    elif y==3 or y==4 or y==5:
        return -5
    elif y==6:
        return -20
    
def preprocess(features):
    Y = list(features['Y'])
    Y_val = list(features['Y_val'])
    C = list(features['C'])
    E = features['E']
    D = features['D']
    n = len(Y)
    i = 0
    while(i<n):
        if np.sum( D[i] ) == 0:
            n = n-1
            del(Y[i])
            del(Y_val[i])
            del(C[i])
            del(E[i])
            del(D[i])
        else:
            Ei = np.hstack( ( np.sum(E[i][:,:4],axis=1).reshape(-1,1), np.mean(E[i][:,4:],axis=1).reshape(-1,1) ) )
            E[i] = Ei
            i += 1
    featureprime = {}
    featureprime['Y'] = np.array(Y)
    featureprime['Y_val'] = np.array(Y_val)
    featureprime['C'] = np.array(C)
    featureprime['E'] = E
    featureprime['D'] = D
    featureprime['Ename'] = ['IIC Ratio','DischargeFreq']
    featureprime['Dname'] = ['NSAED','AED']
    return featureprime

def align_1_nsaed_neigborhood(features,k=5):
    E = features['E']
    D = features['D']
    n = len(E)
    snapshot = {'E':[],'D':[],'idx':[],'t':[]}
    for i in range(0,n):
        Ei = E[i]
        Di1 = D[i][:,0]
        T = len(Di1)
        for t in range(T):
            dit = Di1[t]
            if dit>0:
                st = min( t, k )
                et = min( (T)-t, k+1 )
                dsnap = Di1[t-st:t+et]
                esnap = Ei[t-st:t+et,:]
                snapshot['E'] = snapshot['E'] + [esnap]
                snapshot['D'] = snapshot['D'] + [dsnap]
                snapshot['idx'] = snapshot['idx'] + [i]
                snapshot['t'] = snapshot['t'] + [t]
    return snapshot

def align_0_nsaed_neigborhood(features,k=5):
    E = features['E']
    D = features['D']
    n = len(E)
    snapshot = {'E':[],'D':[],'idx':[],'t':[]}
    for i in range(0,n):
        Ei = E[i]
        Di1 = D[i][:,0]
        T = len(Di1)
        for t in range(T):
            dit = Di1[t]
            if dit==0:
                st = min( t, k )
                et = min( (T)-t, k+1 )
                dsnap = Di1[t-st:t+et]
                esnap = Ei[t-st:t+et,:]
                snapshot['E'] = snapshot['E'] + [esnap]
                snapshot['D'] = snapshot['D'] + [dsnap]
                snapshot['idx'] = snapshot['idx'] + [i]
                snapshot['t'] = snapshot['t'] + [t]
    return snapshot

def filter_eqlen(snapshot,l=11):
    snapshot_f = {'E':[],'D':[],'idx':[],'t':[]}
    D = snapshot['D']
    E = snapshot['E']
    idx = snapshot['idx']
    t = snapshot['t']
    for i in range(len(D)):
        if len(D[i])==l:
            snapshot_f['D'] = snapshot_f['D'] + [D[i]]
            snapshot_f['E'] = snapshot_f['E'] + [E[i]]
            snapshot_f['idx'] = snapshot_f['idx'] + [idx[i]]
            snapshot_f['t'] = snapshot_f['t'] + [t[i]]
    return snapshot_f

def filter_0_eqlen(snapshot,l=11):
    snapshot_f = {'E':[],'D':[],'idx':[],'t':[]}
    D = snapshot['D']
    E = snapshot['E']
    idx = snapshot['idx']
    t = snapshot['t']
    for i in range(len(D)):
        if len(D[i])==l:
            if np.sum(D[i][:(l//2)])==0:
                snapshot_f['D'] = snapshot_f['D'] + [D[i]]
                snapshot_f['E'] = snapshot_f['E'] + [E[i]]
                snapshot_f['idx'] = snapshot_f['idx'] + [idx[i]]
                snapshot_f['t'] = snapshot_f['t'] + [t[i]]
    return snapshot_f

def filter_00_eqlen(snapshot,l=10):
    snapshot_f = {'E':[],'D':[],'idx':[],'t':[]}
    D = snapshot['D']
    E = snapshot['E']
    idx = snapshot['idx']
    t = snapshot['t']
    for i in range(len(D)):
        if len(D[i])==l:
            if np.sum(D[i][:(l//2)])==0 and np.sum(D[i][(l//2)+2:])==0:
                snapshot_f['D'] = snapshot_f['D'] + [D[i]]
                snapshot_f['E'] = snapshot_f['E'] + [E[i]]
                snapshot_f['idx'] = snapshot_f['idx'] + [idx[i]]
                snapshot_f['t'] = snapshot_f['t'] + [t[i]]
    return snapshot_f

def ase(yhat,y):
    e = np.abs(yhat - y)
    scale = 1#+np.mean(np.abs(y[1:] - y[:-1]))
    return e/scale

def delta(i,j,ti,tj,E,D,C,L,Me=None,Mc=None):
    snapD1 = D[i][ti-L:ti,:]
    snapD2 = D[j][tj-L:tj,:]
    snapD1F = D[i][ti+1:ti+L+1,:]
    snapD2F = D[j][tj+1:tj+L+1,:]
#    snapE1 = E[i][ti-L:ti,:]
#    snapE2 = E[j][tj-L:tj,:]
    C1 = C[i]
    C2 = C[j]
    #distance between drug regimes
    dmatch = np.prod(snapD1[:,0] == snapD2[:,0])*np.prod(snapD1F[:,0] == snapD2F[:,0])
    del_d = fastdtw(snapD1[:,1],snapD2[:,1])[0] + ( (1 + np.finfo(float).eps)/ ( np.finfo(float).eps + dmatch) ) - 1
    #distance between states transitions
#    w = np.array([[np.log(i+5*np.e),np.log(i+5*np.e)] for i in range(0,L)])
#    diff_e = w*(snapE1 - snapE2)
#    del_e = 1000000*np.abs(np.mean(snapE1[:,0]) - np.mean(snapE2[:,0])) #np.linalg.norm(np.matmul(diff_e.T,diff_e),ord=1)
    #distance between covariates
    diff_c = np.prod(C1[29:] == C2[29:]) * np.prod( C1[3:27] == C2[3:27]) * np.prod(C1[0]==C2[0]) * np.prod(np.abs(C1[1]-C2[1])<10) * np.prod(np.abs(C1[27]-C2[27])<20)
    del_c = ( (1 + np.finfo(float).eps)/ ( np.finfo(float).eps + diff_c.all() ) ) - 1 #np.matmul(diff_c.T,diff_c)
    #adding up all distances
    del_t = np.abs(ti - tj)
    dist = del_d + del_c + del_t
    return dist

def create_MG(kn,snapshots_1,snapshots_0,E,D,C,L,Me=None,Mc=None):
    idx_0 = np.array(snapshots_0['idx'])
    idx_1 = np.array(snapshots_1['idx'])
    t_0 = snapshots_0['t']
    t_1 = snapshots_1['t']
    E_0 = np.array(snapshots_0['E'])
    E_1 = np.array(snapshots_1['E'])
    D_0 = np.array(snapshots_0['D'])
    D_1 = np.array(snapshots_1['D'])
    control = []
    treated = []
    eff_array = []
    d_array = []
    idx_array = []
    idx_matched = []
    t_array = []
    C_matched = []
    drug_matched = []
    unmatched_idx_treated = []
    for i in range(0,len(idx_1)):
        idx_i = idx_1[i]
        d = np.array([ delta(idx_i,idx_0[j],t_1[i],t_0[j],E,D,C,L//2) for j in range(0,len(idx_0)) ])
        m_idx = np.array([i for i,v in enumerate(d) if v < 5e+1],dtype=np.int)
        if len(m_idx)==0:
            unmatched_idx_treated.append(idx_i)
        else:
        #m_idx = np.argpartition(d, kn)
            if np.max(d[m_idx[:]])<5e+1:
                idx_array.append(idx_i)
                idx_matched.append(idx_0[m_idx[:]])
                t_array.append(t_1[i])
                C_matched.append(C[idx_i])
                drug_matched.append(D_1[i])
                mgi = E_0[m_idx[:],:,:]
#                indpvar = mgi[:,:L//2,0].T
#                response = E_1[i,:L//2,0]
                lm_0 = ensemble.RandomForestRegressor(n_estimators=100).fit( X=mgi[:,:L//2,0].T , y=E_1[i,:L//2,0] )
                lm_1 = ensemble.RandomForestRegressor(n_estimators=100).fit( X=mgi[:,:L//2,1].T , y=E_1[i,:L//2,1] )
#                beta_0 = lm_0.coef_
#                beta0_0 = lm_0.intercept_
                try:
                    mean_mgi_0 = lm_0.predict( mgi[:,:,0].T )#np.average(mgi,weights=beta,axis=0)*np.sum(beta) + beta0
                    mean_mgi_1 = lm_1.predict( mgi[:,:,1].T )#np.average(mgi,weights=beta,axis=0)*np.sum(beta) + beta0
                    mean_mgi = np.array([mean_mgi_0, mean_mgi_1]).T
                except:
                    mean_mgi = np.average(mgi,axis=0)
                d_array.append(d[m_idx[:kn]])
                control.append(mean_mgi)
                treated.append(E_1[i,:,:])
                eff = E_1[i,:,:] - mean_mgi
                eff_array.append(eff)
    return np.array(control), np.array(treated), np.array(eff_array) , np.array(idx_array), np.array(d_array), np.array(C_matched), np.array(drug_matched), np.array(t_array), unmatched_idx_treated
    

def dist_snapshot(s1,s2):
    T = len(s1)
    diff = s1[:T//2,:]-s2[:T//2,:]
    d = np.matmul(diff.T,diff)
    return np.linalg.norm(d,ord=1)
    

##SETTING UP THE DATA
Y = features['Y']
C = features['C']
Eprime = features['E']
E = [Eprime[i][:,np.array([0,1,2,3,5,7,9])] for i in range(0,len(Eprime))]
Dprime = features['D']
D = [Dprime[i][:,np.array([0,1])] for i in range(0,len(Dprime))]
Enameprime = np.array(features['Enames'])
Dnameprime = np.array(features['Dnames'])
Ename = Enameprime[np.array([0,1,2,3,5,7,9])]
Dname = Dnameprime[np.array([0,1])]
Cname = features['Cnames']
Y_val = np.array(list(map(f,Y)))
features['Y_val'] = Y_val

featureprime = {}
featureprime['Y'] = Y
featureprime['C'] = C
featureprime['E'] = E
featureprime['D'] = D
featureprime['Ename'] = Ename
featureprime['Dname'] = Dname
featureprime['Cname'] = Cname
featureprime['Y_val'] = Y_val

featureprime = preprocess(featureprime)

##MAKING SNAPSHOTS FOR CASE CONTROL
times = int(time.time())

k = 20
l = 2*k + 1

snapshots_nsaed_1 = align_1_nsaed_neigborhood(featureprime,k=k)
snapshots_nsaed_0 = align_0_nsaed_neigborhood(featureprime,k=k)

snapshots_nsaed_eq_1 = filter_eqlen(snapshots_nsaed_1,l=l)
snapshots_nsaed_eq_0 = filter_eqlen(snapshots_nsaed_0,l=l)

snapshots_nsaed_f_1 = filter_0_eqlen(snapshots_nsaed_1,l=l)
snapshots_nsaed_f_0 = filter_00_eqlen(snapshots_nsaed_0,l=l)


##CREATING MATCHED GROUPS 00
control, treated, effect, idx_array, d_array, c_matched, drug_matched, t_array, unmatched_idx_treated = create_MG(kn=4,snapshots_1=snapshots_nsaed_f_1,snapshots_0=snapshots_nsaed_f_0,E=featureprime['E'],D=featureprime['D'],C=featureprime['C'],L=l)

fig = plt.figure(figsize=(8.75,7))
plt.plot(np.mean(treated[:,:,0],axis=0))
plt.plot(np.mean(control[:,:,0],axis=0))
plt.plot(np.mean(effect[:,:,0],axis=0))
fig.savefig('E0_00_treatment.png')


fig = plt.figure(figsize=(8.75,7))
plt.plot(np.mean(treated[:,:,1],axis=0))
plt.plot(np.mean(control[:,:,1],axis=0))
plt.plot(np.mean(effect[:,:,1],axis=0))
fig.savefig('E1_00_treatment.png')


fig = plt.figure(figsize=(8.75,7))
eff_0 = effect[:,:,0]
times = np.array([list(np.arange(0,2*k+1)) for u in range(0,len(eff_0))],dtype=np.float64)
plt.scatter(x=times,y=eff_0,alpha=0.01)
fig.savefig('E0_00_treatment_scatter.png')

#fig = plt.figure(figsize=(8.75,7))
#lm = svm.SVR().fit(np.mean(treated[:,:k,0],axis=1).reshape(-1,1),eff_0[:,25])
#x = np.linspace(np.min(np.mean(treated[:,:k,0],axis=1)),np.max(np.mean(treated[:,:k,0],axis=1))).reshape(-1,1)
#y = lm.predict(x)
#plt.scatter(np.mean(treated[:,:k,0],axis=1),eff_0[:,25],c=np.log(1+np.mean(d_array,axis=1)),alpha=0.3,cmap=plt.cm.tab20b)
#plt.plot(x,y)
#plt.ylabel('Estimated Drug Effect after 50 mins')
#plt.xlabel('Avg. Pre-drug IIC Ratio')
#plt.colorbar()
#fig.savefig('Effect_vs_E0_full_treatment_scatter.png')

fig = plt.figure(figsize=(8.75,7))
sns.regplot(x=np.mean(treated[:,:k,0],axis=1) , y=np.mean(eff_0[:,k:],axis=1),order=2)
plt.ylabel('Estimated Drug Effect after 50 mins')
plt.xlabel('Time when NSAED was given')
fig.savefig('Effect_vs_E0_full_treatment_scatter.png')

D_treated = np.array(D)
D_treated = D_treated[idx_array]

E0_mean = np.array([np.mean(E[idx_array[i]][:t_array[i],0]) for i in range(len(idx_array))])
df = pd.DataFrame()
df['Pre-NSAED IIC'] = E0_mean
df['Effect'] = np.mean(effect[:,k:,0],axis=1)
df['time']=t_array
df['idx_unit'] = idx_array
df_pruned = df.loc[df['Pre-NSAED IIC']>0.01]
df_spooky = df_pruned.loc[df_pruned['Effect']>0.3]
order = 2
fs = 30.0       # sample rate, Hz
cutoff = 3.667
for i in df_spooky['idx_unit']:
    Espooky0 = E[i][:,0]
    Espooky0_f = butter_lowpass_filter(Espooky0,cutoff,fs,order=2)
    Dspooky0 = D[i][:,0]
    Dspooky1 = D[i][:,1]
    fig = plt.figure(figsize=(20,20))
    plt.plot(Espooky0_f*(Espooky0_f>0),alpha=0.9)
    for tp in range(0,len(Dspooky0)):
        if Dspooky0[tp]>0.5:
            plt.axvline(x=tp,c='red',lw=2)
    plt.fill_between(x=np.arange(0,len(Dspooky1)),y1=Dspooky1,alpha=0.25,color='#FF6347')
    fig.savefig('spooky_%d.png'%(i))

    

##CREATING MATCHED GROUPS FULL

#control, treated, effect, idx_array, d_array, c_matched, drug_matched, t_array, unmatched_idx_treated = create_MG(kn=3,snapshots_1=snapshots_nsaed_eq_1,snapshots_0=snapshots_nsaed_eq_0,E=featureprime['E'],D=featureprime['D'],C=featureprime['C'],L=l)
#
#fig = plt.figure(figsize=(8.75,7))
#plt.plot(np.mean(treated[:,:,0],axis=0))
#plt.plot(np.mean(control[:,:,0],axis=0))
#plt.plot(np.mean(effect[:,:,0],axis=0))
#fig.savefig('E0_full_treatment.png')
#
#
#fig = plt.figure(figsize=(8.75,7))
#plt.plot(np.mean(treated[:,:,1],axis=0))
#plt.plot(np.mean(control[:,:,1],axis=0))
#plt.plot(np.mean(effect[:,:,1],axis=0))
#fig.savefig('E1_full_treatment.png')
#
#
#fig = plt.figure(figsize=(8.75,7))
#eff_0 = effect[:,:,0]
#times = np.array([list(np.arange(0,2*k+1)) for u in range(0,len(eff_0))],dtype=np.float64)
#plt.scatter(x=times,y=eff_0,alpha=0.01)
#fig.savefig('E0_full_treatment_scatter.png')
#
#fig = plt.figure(figsize=(8.75,7))
#lm = svm.SVR().fit(np.mean(treated[:,:k,0],axis=1).reshape(-1,1),eff_0[:,25])
#x = np.linspace(np.min(np.mean(treated[:,:k,0],axis=1)),np.max(np.mean(treated[:,:k,0],axis=1))).reshape(-1,1)
#y = lm.predict(x)
#plt.scatter(np.mean(treated[:,:k,0],axis=1),eff_0[:,25],c=np.log(1+np.mean(d_array,axis=1)),alpha=0.2)
#plt.plot(x,y)
#fig.savefig('Effect_vs_E0_full_treatment_scatter.png')


#
#R = np.zeros((l,l))
#
#Dsnap_1 = snapshots_nsaed_f_1['D']
#Esnap_1 = snapshots_nsaed_f_1['E']
#idxsnap_1 = snapshots_nsaed_f_1['idx']
#
#Dsnap_0 = snapshots_nsaed_f_0['D']
#Esnap_0 = snapshots_nsaed_f_0['E']
#idxsnap_0 = snapshots_nsaed_f_0['idx']
#
#for i in range(0,len(Dsnap_1)):
#    R += np.matmul(Esnap_1[i][:,0].reshape(-1,1),Dsnap_1[i].reshape(1,-1))
#R = R/len(Dsnap_1)
#
##treated case control
#Esnap_1 = np.array(Esnap_1)
#E11 = Esnap_1[:,:,0]
#E21 = Esnap_1[:,:,1]
#E11_mean = np.mean(E11,axis=0)
#E21_mean = np.mean(E21,axis=0)
#E11_std = np.std(E11,axis=0)/10
#E21_std = np.std(E21,axis=0)/10
#
##untreated case control
#Esnap_0 = np.array(Esnap_0)
#E10 = Esnap_0[:,:,0]
#E20 = Esnap_0[:,:,1]
#E10_mean = np.mean(E10,axis=0)
#E20_mean = np.mean(E20,axis=0)
#E10_std = np.std(E10,axis=0)/10
#E20_std = np.std(E20,axis=0)/10
#
#
#fig = plt.figure(figsize=(8.75,7))
#plt.plot(E11_mean)
#plt.fill_between(np.arange(0,l), E11_mean-E11_std, E11_mean+E11_std,alpha=0.3)
#plt.plot(E10_mean)
#plt.fill_between(np.arange(0,l), E10_mean-E10_std, E10_mean+E10_std,alpha=0.3)
#plt.xticks(np.arange(0,l),np.arange(-k,k))
#plt.axvline(x=k,c='r',ls='--')
#plt.xlabel('T')
#plt.ylabel('IIC ratio')
#fig.savefig('E1_snapshot_1_0.png')
#
#fig = plt.figure(figsize=(8.75,7))
#plt.plot(E21_mean)
#plt.fill_between(np.arange(0,l), E21_mean-E21_std, E21_mean+E21_std,alpha=0.3)
#plt.plot(E20_mean)
#plt.fill_between(np.arange(0,l), E20_mean-E20_std, E20_mean+E20_std,alpha=0.3)
#plt.xticks(np.arange(0,l),np.arange(-k,k))
#plt.axvline(x=k,c='r',ls='--')
#plt.xlabel('T')
#plt.ylabel('mean discharge')
#fig.savefig('E2_snapshot_1_0.png')
#
#
###MATCH
#control_idx = []
#treated_idx = []
#eff_array = []
#d_array = []
#control = []
#treated = []
#for i in range(0,500):
#    kn=5
#    s1 = Esnap_1[i,:,:]
#    d = np.array([ dist_snapshot(s1,s2) for s2 in Esnap_0 ])
#    d_1 = np.array([ dist_snapshot(s1,s2) for s2 in Esnap_1 ])
#    s2_m_idx = np.argpartition(d, kn)
#    s1_m_idx = np.argpartition(d_1, 2)
##    s2_m_idx = np.argmin(d)
#    if d[s2_m_idx[0]]<1000:
#        d_array.append(d[s2_m_idx[:kn]])
#        s2_m = np.mean(Esnap_0[s2_m_idx[:kn],:,:],axis=0)
#        s1_m = np.mean(Esnap_1[s1_m_idx[:2],:,:],axis=0)
#        control.append(s2_m)
#        treated.append(s1_m)
#        plt.plot(s1_m[:,0],color='#1f77b4',alpha=0.1)
#        plt.plot(s2_m[:,0],color='#ff7f0e',alpha=0.1)
#        eff = s1_m - s2_m
#        eff_array.append(eff)
#        control_idx.append(s2_m_idx)
#        treated_idx.append(i)
#
#treated = np.array(treated)
#control = np.array(control)
#eff_array = np.array(eff_array)
#d_array = np.array(d_array)
##treated case control
#treated = np.array(treated)
#treated1 = treated[:,:,0]
#treated2 = treated[:,:,1]
#treated1_mean = np.mean(treated1,axis=0)
#treated2_mean = np.mean(treated2,axis=0)
#treated1_std = np.std(treated1,axis=0)/30
#treated2_std = np.std(treated2,axis=0)/30
#
##untreated case control
#control = np.array(control)
#control1 = control[:,:,0]
#control2 = control[:,:,1]
#control1_mean = np.mean(control1,axis=0)
#control2_mean = np.mean(control2,axis=0)
#control1_std = np.std(control1,axis=0)/30
#control2_std = np.std(control2,axis=0)/30
#
##effect
#eff_array = np.array(eff_array)
#eff_array1 = eff_array[:,:,0]
#eff_array2 = eff_array[:,:,1]
#eff_array1_mean = np.mean(eff_array1,axis=0)
#eff_array2_mean = np.mean(eff_array2,axis=0)
#eff_array1_std = np.std(eff_array1,axis=0)/30
#eff_array2_std = np.std(eff_array2,axis=0)/30
#
#svr_rbf_t1 = SVR(kernel='rbf', C=100, epsilon=0.01)
#svr_rbf_c1 = SVR(kernel='rbf', C=100, epsilon=0.005)
#svr_rbf_e1 = SVR(kernel='rbf',  C=100, epsilon=0.005)
#
#tl = np.array(list(np.arange(0,l))).reshape(-1,1)
#svr_rbf_t1 = svr_rbf_t1.fit(tl,treated1_mean)
#svr_rbf_c1 = svr_rbf_c1.fit(tl,control1_mean)
#svr_rbf_e1 = svr_rbf_e1.fit(tl,eff_array1_mean)
#tlp = np.array(list(np.linspace(0, l, num=10000))).reshape(-1,1)
#
##plt.plot(tlp,svr_rbf_t1.predict(tlp))
##plt.plot(tlp,svr_rbf_c1.predict(tlp))
##plt.plot(svr_rbf_e1.predict(tl))
##plt.plot(treated1_mean)
##plt.plot(control1_mean)
##plt.plot(eff_array1_mean)
#
#
#fig = plt.figure(figsize=(80.75,70))
#plt.rcParams.update({'font.size': 18.5})
#plt.title('Case Control: Treatment Effect of NSAED')
#plt.plot(treated1_mean,linewidth=3)
##plt.fill_between(np.arange(0,l), treated1_mean-treated1_std, treated1_mean+treated1_std,alpha=0.3)
#plt.plot(control1_mean,linewidth=3)
##plt.fill_between(np.arange(0,l), control1_mean-control1_std, control1_mean+control1_std,alpha=0.3)
#plt.plot(eff_array1_mean,linewidth=3)
##plt.fill_between(np.arange(0,l), eff_array1_mean-eff_array1_std, eff_array1_mean+eff_array1_std,alpha=0.3)
#plt.xlim((1,l-1))
#plt.xticks(np.arange(1,l+1),np.arange(-(k),k+1),rotation=75)
#plt.axvline(x=k+1,c='r',ls='--')
#plt.axhline(0,c='black',ls='-')
#plt.xlabel('T')
#plt.ylabel('IIC ratio')
#plt.legend(['Avg. Treated TS','Avg. Control TS','Avg. Causal Effect TS'])
#fig.savefig('effect_1.png')
#
#fig = plt.figure(figsize=(80.75,70))
#plt.rcParams.update({'font.size': 18.5})
#plt.title('Case Control: Treatment Effect of NSAED')
#plt.plot(np.arange(k+1,len(eff_array1_mean)),eff_array1.T[k+1:,:],alpha=0.3)
#plt.plot(eff_array1_mean,color='black',linestyle='dashed',linewidth=5)
##plt.fill_between(np.arange(0,l), eff_array1_mean-eff_array1_std, eff_array1_mean+eff_array1_std,alpha=0.3)
#plt.xlim((1,l-1))
#plt.xticks(np.arange(1,l+1),np.arange(-(k),k+1),rotation=75)
#plt.axvline(x=k+1,c='r',ls='--')
#plt.axhline(0,c='black',ls='-')
#plt.xlabel('T')
#plt.ylabel('IIC ratio')
#fig.savefig('conditional_effect_1.png')
#
#fig = plt.figure(figsize=(80.75,70))
#plt.rcParams.update({'font.size': 18.5})
#plt.plot(treated2_mean,linewidth=3)
##plt.fill_between(np.arange(0,l), treated2_mean-treated2_std, treated2_mean+treated2_std,alpha=0.3)
#plt.plot(control2_mean,linewidth=3)
##plt.fill_between(np.arange(0,l), control2_mean-control1_std, control2_mean+control1_std,alpha=0.3)
#plt.plot(eff_array2_mean,linewidth=3)
##plt.fill_between(np.arange(0,l), eff_array2_mean-eff_array2_std, eff_array2_mean+eff_array2_std,alpha=0.3)
#plt.xticks(np.arange(1,l+1),np.arange(-(k),k+1))
#plt.axvline(x=k+1,c='r',ls='--')
#plt.axhline(0,c='black',ls='-')
#plt.xlabel('T')
#plt.ylabel('Discharge Frequency')
#fig.savefig('effect_2.png')
#
#te27 = eff_array1[:,27]
#tidx = snapshots_nsaed_f_1['idx']
#c27 = C[tidx,:]
#y27 = Y[tidx]
#c27_500 = c27[:500,:]
#y27_500 = y27[:500]
#cov_eff_m = linearmodel.ARDRegression()
##scores = cross_validate(cov_eff_m,c27_500,te27,cv=3)
#cov_eff_m = cov_eff_m.fit(c27_500,te27)
#for i in range(len(cov_eff_m.coef_)):
#    print( (Cname[i],cov_eff_m.coef_[i]) )
#
##print(scores)
#fig = plt.figure(figsize=(80.75,70))
#plt.rcParams.update({'font.size': 18.5})
#plt.scatter(y27_500+np.random.normal(0,0.1,size=len(te27)),te27,c=te27,alpha=0.1,s=60,cmap=plt.cm.winter)
#plt.axhline(np.median(te27),c='black')
#plt.text(6.2,np.median(te27)+0.01, 'TE median=%.4f'%(np.median(te27)))#, transform=trans)
#plt.axhline(np.mean(te27))
#plt.text(6.2,np.mean(te27)-0.05, 'TE mean=%.4f'%(np.mean(te27)))#, transform=trans)
#plt.xlabel('Observed MRS Outcome')
#plt.ylabel('Estimated TE at t=+60 mins')
#fig.savefig('te27_outcome.png')

#for i in range(len(cov_eff_m.coef_)):
#    for j in range(len(cov_eff_m.coef_)):
#        fig = plt.figure(figsize=(80.75,70))
#        plt.rcParams.update({'font.size': 18.5})
#        plt.scatter(c27_500[:,i]+np.random.normal(0,0.1,size=len(te27)),c27_500[:,j]+np.random.normal(0,0.1,size=len(te27)),c=te27,alpha=0.35,s=60,cmap=plt.cm.winter)
#        #plt.axhline(np.median(te27),c='black')
#        #plt.text(1.2,np.median(te27)+0.01, 'TE median=%.4f'%(np.median(te27)))#, transform=trans)
#        plt.axhline(np.mean(te27))
#        plt.text(0.5,np.mean(te27)-0.05, 'TE mean=%.4f'%(np.mean(te27)),alpha=0.2)#, transform=trans)
#        plt.xlabel(Cname[i])
#        plt.ylabel('Estimated TE at t=+60 mins')
#        plt.title('ARD Regression Coeff=%.4f'%(cov_eff_m.coef_[i]))
#    fig.savefig('te27_%s_%s.png'%(Cname[i].replace('/',''),Cname[j].replace('/','')))
    
#fig = plt.figure(figsize=(8.75,7))
#plt.plot(E21_mean)
#plt.fill_between(np.arange(0,l), E21_mean-E21_std, E21_mean+E21_std,alpha=0.3)
#plt.plot(E20_mean)
#plt.fill_between(np.arange(0,l), E20_mean-E20_std, E20_mean+E20_std,alpha=0.3)
#plt.xticks(np.arange(0,l),np.arange(-k,k))
#plt.axvline(x=k,c='r',ls='--')
#plt.xlabel('T')
#plt.ylabel('mean discharge')
#fig.savefig('E2_snapshot_1_0.png')
    
#    plt.plot(s1[:,0],alpha=0.1)
#    plt.plot(s2_m[:,0],alpha=0.1)


'''
##off_policy
torch.manual_seed(0)
np.random.seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

def reward_mgh(Eit,Dit):
    return 0

lag = 10
forecast = 20
Ename = featureprime['Ename']
Dname = featureprime['Dname']
QL_off_policy = QLearning(state_space_dimension=7,action_space_dimension=2,covariate_space_dimension=61, off_policy=True, reward = reward_mgh, discount=1, model='random-forest')
#QL_off_policy = QL_off_policy.fit(featureprime,verbose=False,itr=1)
QL_off_policy = QL_off_policy.fit_simulator(featureprime, lag=lag, forecast=forecast, verbose=False)

forecast_range = 50
ase_df = pd.DataFrame()
for case in range(0,1000):
    try:
        init_features = {}
        init_features['E'] = E[case][:QL_off_policy.lag,:]
        init_features['D'] = D[case][:QL_off_policy.lag-1,:]
        init_features['C'] = C[case]
        df_temp = pd.DataFrame()
        Esim, Dsim = QL_off_policy.run_simulator(init_features, D_forecast=D[case][QL_off_policy.lag-1:,:], T=forecast_range)
        baseline_iic = list(E[case][:QL_off_policy.lag,0]) + list(E[case][QL_off_policy.lag-1,0]*np.ones((forecast_range,)))
        baseline_disfreq = list(E[case][:QL_off_policy.lag,1]) + list(E[case][QL_off_policy.lag-1,1]*np.ones((forecast_range,)))
        df_temp['t'] = np.arange(-QL_off_policy.lag+1,forecast_range+1)*10
        df_temp['ASE_IIC_Ratio'] = ase(Esim[:,0],E[case][:len(Esim),0])
        df_temp['ASE_Discharge_Freq'] = ase(Esim[:,1],E[case][:len(Esim),1])
        df_temp['ASE_baseline_IIC_Ratio'] = ase(baseline_iic,E[case][:len(baseline_iic),0])
        df_temp['ASE_baseline_Discharge_Freq'] = ase(baseline_disfreq,E[case][:len(baseline_disfreq),1])
        ase_df = ase_df.append(df_temp)
    except:
        continue
    
fig = plt.figure(figsize=(8.75,7))
#sns.lineplot(x='t',y='ASE_IIC_Ratio',data=ase_df)
sns.lineplot(x='t',y='ASE_Discharge_Freq',data=ase_df)
sns.lineplot(x='t',y='ASE_baseline_Discharge_Freq',data=ase_df,dashes=True)
#sns.lineplot(x='t',y='ASE_baseline_IIC_Ratio',data=ase_df,dashes=True)
plt.ylabel('ASE')
plt.xlabel('time (minutes)')
plt.legend(['Baseline IIC Ratio','IIC Ratio'])
fig.savefig('ASE_IIC_Ratio_%d.png'%(forecast_range))
'''