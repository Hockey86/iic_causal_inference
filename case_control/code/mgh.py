# -*- coding: utf-8 -*-
"""MGH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1328EbQmeZ9maTWn5AGkikTTttcHecddR

# Setting up
"""

import sys
import os
import time
#!{sys.executable} -m pip install keras-rl

import numpy as np
import pickle

import neuralnet 

import torch
import torch.nn as nn

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns

import sklearn.linear_model as linearmodel
import sklearn.ensemble as ensemble
import sklearn.svm as svm
from bartpy.sklearnmodel import SklearnModel as bart
from sklearn.model_selection import cross_validate
from sklearn.cluster import DBSCAN

import scipy.optimize as opt

#import gym
#from gym import spaces
#from gym.utils import seeding
#
#from keras.models import Sequential
#from keras.layers import Dense, Activation, Flatten
#from keras.optimizers import Adam
#
#from rl.agents.dqn import DQNAgent
#from rl.policy import EpsGreedyQPolicy
#from rl.memory import SequentialMemory

#import pickle
#
#from sklearn.ensemble import RandomForestRegressor as RFR
#from sklearn.svm import SVR
#
#import sklearn.linear_model as lm
#
#import sklearn.neural_network as nn

#import torch
#import torch.nn as nn
window = 600
datafile = open('../IICdata/preprocessed_dataset_all.pickle','rb')
data = pickle.load( datafile , encoding='latin')

big_datafile = open('../IICdata/for_Harsh/data/big_dataset.pickle','rb')
big_dataset = pickle.load( big_datafile , encoding='latin')

featurefile = open('../IICdata/for_Harsh/data/features_window_%d.pickle'%(window),'rb')
features = pickle.load( featurefile , encoding='latin')



"""# Q-Learning
Defining the model
"""



class QLearning:
    def __init__( self, state_space_dimension, action_space_dimension, covariate_space_dimension, reward = ( lambda x: 0 ), discount=1, off_policy=True, model='ridge' ):
        self.state_space_dimension = state_space_dimension
        self.action_space_dimension = action_space_dimension
        self.covariate_space_dimension = covariate_space_dimension
        self.model = model
        if self.model=='bart':
            self.Q = bart()
            self.A = lambda s1,c: opt.minimize(fun = lambda a1: -1*self.Q.predict([np.hstack((s1,a1,c))])[0], x0 = np.zeros((self.action_space_dimension,)), method='CG').x
            self.S = bart()
        if self.model=='ridge':
            self.Q = linearmodel.Ridge()
            self.A = lambda s1,c: opt.minimize(fun = lambda a1: -1*self.Q.predict([np.hstack((s1,a1,c))])[0], x0 = np.zeros((self.action_space_dimension,)), method='CG').x
            self.S = linearmodel.Ridge()
        if self.model=='random-forest':
            self.Q = ensemble.RandomForestRegressor()
            self.A = lambda s1,c: opt.minimize(fun = lambda a1: -1*self.Q.predict([np.hstack((s1,a1,c))])[0], x0 = np.zeros((self.action_space_dimension,)), method='CG').x
            self.S = ensemble.RandomForestRegressor()
        if self.model=='neural-net':
            self.Q = neuralnet.Net(self.state_space_dimension+self.action_space_dimension+self.covariate_space_dimension,1,[16],isOutputPositive=0)
            self.A = neuralnet.Net(self.state_space_dimension+self.action_space_dimension+self.covariate_space_dimension,self.action_space_dimension,[48],isOutputPositive=1)
            self.S = neuralnet.Net(self.state_space_dimension+self.action_space_dimension+self.covariate_space_dimension,self.state_space_dimension,[32],isOutputPositive=1)
        self.off_policy = off_policy
        self.discount = discount
        self.reward = reward
        self.lag = 1
        self.forecast = 1
    
    def fit( self, features, learningrate=0.05, losscutoff=1001296, verbose=False, itr=5):
        Y = features['Y']
        C = features['C']
        E = features['E']
        D = features['D']
        Y_val = features['Y_val']
        N = len(Y)
        if self.model=='ridge' or self.model=='random-forest':
            #first fit the last step
            XE = np.array([ E[i][-1] for i in range(0,N) ])
            XD = np.array([ D[i][-1] for i in range(0,N) ])
            X = np.hstack((XE,XD,C))
            self.Q = self.Q.fit(X,Y_val)
            
            #now learn other steps
            for itrtn in range(0,itr):
                for i in range(0,N):
                    T,ps = E[i].shape
                    if T>2:
                        XCi = np.array([ C[i] for t in range(0,T) ])
                        Xi = np.hstack((E[i],D[i],XCi))
                        XE1i = E[i][1:,:]
                        if self.off_policy:
                            A1i = np.array([ self.A( E[i][t+1],C[i] ) for t in range(0,T-1) ])
                        else:
                            A1i = D[i][1:,:]
                        #print((np.shape(XE1i),np.shape(A1i),np.shape(XCi[:T-1,:])) )
                        X1i = np.hstack((XE1i,A1i,XCi[:T-1,:]))
                        Y_vali = np.hstack( ( self.discount*self.Q.predict(X1i) + np.array([self.reward(E[i][t],D[i][t]) for t in range(0,T-1)]), np.array([ Y_val[i] ]) ) )
                        if i==0:
                            X = Xi
                            Y_val = Y_vali
                        else:
                            X = np.vstack((X,Xi))
                            Y_val = np.hstack((Y_val,Y_vali))
                self.Q = self.Q.fit(X,Y_val)
        return self
                
    def fit_simulator( self, features, lag=1, forecast=5, learningrate=0.05, losscutoff=1001296, verbose=False):
        Y = features['Y']
        C = features['C']
        E = features['E']
        D = features['D']
        N = len(Y)
        self.lag = lag
        self.forecast = forecast
        if self.model=='ridge' or self.model=='random-forest':
            X = []
            Out = []
            for itrtn in range(0,1):
                for i in range(0,N): #for a particular patient:
                    T,ps = E[i].shape #time they are in observation
                    if T>lag+1: #checking if they are long enough there
                        for t in range(0,T-(lag+forecast)+1):
                            ex, dx, cx = E[i][t:t+lag,:].reshape(1,-1)[0], D[i][t:t+lag,:].reshape(1,-1)[0], C[i]
                            Xit = np.hstack((ex,dx,cx)) 
                            Yit = E[i][t+lag:t+lag+forecast,:].reshape(1,-1)[0]
                            X.append(Xit)
                            Out.append(Yit)
            X = np.array(X)
            sample_weights = 10000*(np.sum(X[:,2*lag:4*lag],axis=1)) + 1
            scores = cross_validate(self.S,np.array(X),np.array(Out),cv=5)
            print(np.mean(scores['test_score']))
            self.S = self.S.fit(np.array(X),np.array(Out),sample_weight=sample_weights)  
            print(self.S.score(np.array(X),np.array(Out)))                  
        return self
    
    def run_simulator(self, init_features,T=20):
        C = init_features['C'] #C, only for one person, length = 1
        E = init_features['E'] #E, only for one person, length = lag
        D = init_features['D'] #D, only for one person, length = lag-1    
        lag = self.lag
        for t in range(0,T):
            D1 = self.A(E[-1,:],C)
            if len(D)>0:
                D = np.vstack((D,D1))
            else:
                D = D1.reshape(1,-1)
            ex, dx, cx = E[-lag:,:].reshape(1,-1)[0], D[-lag:,:].reshape(1,-1)[0], C
            Xit = np.hstack((ex,dx,cx)) 
            E1 = self.S.predict(Xit.reshape(1,-1))
            E1 = E1.reshape((self.forecast,2))
            E = np.vstack((E,E1[0,:].reshape(1,-1)))
        return E,D
            
    
def f(y):
    if y==0 or y==1 or y==2:
        return 0
    elif y==3 or y==4 or y==5:
        return -5
    elif y==6:
        return -20
    
def preprocess(features):
    Y = list(features['Y'])
    Y_val = list(features['Y_val'])
    C = list(features['C'])
    E = features['E']
    D = features['D']
    n = len(Y)
    i = 0
    while(i<n):
        if np.sum( D[i] ) == 0:
            n = n-1
            del(Y[i])
            del(Y_val[i])
            del(C[i])
            del(E[i])
            del(D[i])
        else:
            Ei = np.hstack( ( np.sum(E[i][:,:4],axis=1).reshape(-1,1), np.mean(E[i][:,4:],axis=1).reshape(-1,1) ) )
            E[i] = Ei
            i += 1
    featureprime = {}
    featureprime['Y'] = np.array(Y)
    featureprime['Y_val'] = np.array(Y_val)
    featureprime['C'] = np.array(C)
    featureprime['E'] = E
    featureprime['D'] = D
    featureprime['Ename'] = ['IIC Ratio','DischargeFreq']
    featureprime['Dname'] = ['NSAED','AED']
    featureprime['Cname'] = features['Cname']
    return featureprime

def align_nsaed_neigborhood(features,k=5):
    E = features['E']
    D = features['D']
    n = len(E)
    snapshot = {'E':[],'D':[]}
    for i in range(0,n):
        Ei = E[i]
        Di1 = D[i][:,0]
        T = len(Di1)
        for t in range(T):
            dit = Di1[t]
            if dit>0:
                st = min( t, k )
                et = min( (T)-t, k )
                dsnap = Di1[t-st:t+et]
                esnap = Ei[t-st:t+et,:]
                snapshot['E'] = snapshot['E'] + [esnap]
                snapshot['D'] = snapshot['D'] + [dsnap]
    return snapshot

def filter_eqlen(snapshot,l=10):
    snapshot_f = {'E':[],'D':[]}
    D = snapshot['D']
    E = snapshot['E']
    for i in range(len(D)):
        if len(D[i])==l:
            snapshot_f['D'] = snapshot_f['D'] + [D[i]]
            snapshot_f['E'] = snapshot_f['E'] + [E[i]]
    return snapshot_f

Y = features['Y']
C = features['C']
Eprime = features['E']
E = [Eprime[i][:,np.array([0,1,2,3,5,7,9])] for i in range(0,len(Eprime))]
Dprime = features['D']
D = [Dprime[i][:,np.array([0,1])] for i in range(0,len(Dprime))]
Enameprime = np.array(features['Enames'])
Dnameprime = np.array(features['Dnames'])
Ename = Enameprime[np.array([0,1,2,3,5,7,9])]
Dname = Dnameprime[np.array([0,1])]
Cname = features['Cnames']
Y_val = np.array(list(map(f,Y)))
features['Y_val'] = Y_val

featureprime = {}
featureprime['Y'] = Y
featureprime['C'] = C
featureprime['E'] = E
featureprime['D'] = D
featureprime['Ename'] = Ename
featureprime['Dname'] = Dname
featureprime['Cname'] = Cname
featureprime['Y_val'] = Y_val

featureprime = preprocess(featureprime)
Y = featureprime['Y']
C = featureprime['C']
E = featureprime['E']
D = featureprime['D']
Ename = featureprime['Ename']
Dname = featureprime['Dname']
Cname = featureprime['Cname']
Y_val = featureprime['Y_val']
times = int(time.time())

#discrete = list(np.arange(0,len(Cname)))
#discrete.remove(1) #age
#discrete.remove(25 )
#discrete.remove(27) #BP
#discrete.remove(31)
#for i in range(0,len(Cname)):
#    Cnamei = Cname[i].replace('/','')
#    fig = plt.figure(figsize=(8.75,7))
#    if i in discrete:
#        sns.distplot(features['C'][:,i],hist=True,kde=False,norm_hist=True,kde_kws={'shade': True},hist_kws={"alpha": 0.2})
#        sns.distplot(featureprime['C'][:,i],hist=True,kde=False,norm_hist=True,kde_kws={'shade': True},hist_kws={"alpha": 0.2})
#    else:
#        sns.distplot(features['C'][:,i],hist=True,kde=True,norm_hist=True,kde_kws={'shade': True},hist_kws={"alpha": 0.2})
#        sns.distplot(featureprime['C'][:,i],hist=True,kde=True,norm_hist=True,kde_kws={'shade': True},hist_kws={"alpha": 0.2})
#    plt.ylabel('Estimated PDF')
#    plt.xlabel(Cname[i])
#    plt.title('PDF '+Cname[i])
#    plt.legend(['Original','Pruned'])
#    fig.savefig('PDF'+Cnamei+'.png')

k = 20
l = 2*k

snapshots_nsaed = align_nsaed_neigborhood(featureprime,k=k)
snapshots_nsaed_f = filter_eqlen(snapshots_nsaed,l=l)
R = np.zeros((l,l))
Dsnap = snapshots_nsaed_f['D']
Esnap = snapshots_nsaed_f['E']
for i in range(0,len(Dsnap)):
    R += np.matmul(Esnap[i][:,0].reshape(-1,1),Dsnap[i].reshape(1,-1))
R = R/len(Dsnap)


fig = plt.figure(figsize=(8.75,7))
sns.heatmap(R)
plt.xticks(np.arange(0,l),np.arange(-k,k))
plt.yticks(np.arange(0,l),np.arange(-k,k))
plt.axvline(x=k,c='r',ls='--',ymin=-2,ymax=12)
plt.xlabel('D')
plt.ylabel('E')
fig.savefig('cross_correlation_matrix.png')

Esnap_1 = np.array(Esnap)
E1 = Esnap_1[:,:,0]
E2 = Esnap_1[:,:,1]
E1_mean = np.mean(E1,axis=0)
E2_mean = np.mean(E2,axis=0)
E1_std = np.std(E1,axis=0)/10
E2_std = np.std(E2,axis=0)/10

fig = plt.figure(figsize=(8.75,7))
plt.plot(E1_mean)
plt.fill_between(np.arange(0,l), E1_mean-E1_std, E1_mean+E1_std,alpha=0.3)
plt.xticks(np.arange(0,l),np.arange(-k,k))
plt.axvline(x=k,c='r',ls='--')
plt.xlabel('T')
plt.ylabel('E IIC ration')
fig.savefig('E1_snapshot.png')

fig = plt.figure(figsize=(8.75,7))
plt.plot(E2_mean)
plt.fill_between(np.arange(0,l), E2_mean-E2_std, E2_mean+E2_std,alpha=0.3)
plt.xticks(np.arange(0,l),np.arange(-k,k))
plt.axvline(x=k,c='r',ls='--')
plt.xlabel('T')
plt.ylabel('E mean discharge')
fig.savefig('E2_snapshot.png')



#
#from sklearn.ensemble import RandomForestRegressor as RFR
#DS = []
#for i in range(0,len(E1)):
#    for t in range(0,20):
#        DS.append([t,E1[i][t]])
#DS = np.array(DS)     
#mdl = RFR(n_estimators=1000)
#mld = mdl.fit(DS[:,0].reshape(-1,1),DS[:,1])
#
#DS = []
#for i in range(0,len(E2)):
#    for t in range(0,20):
#        DS.append([t,E2[i][t]])
#DS = np.array(DS)     
#mdl2 = RFR(n_estimators=1000)
#ml2 = mdl2.fit(DS[:,0].reshape(-1,1),DS[:,1])
#
#E1_pred = np.array([mdl.predict(np.array([[t]])) for t in range(0,20)])[:,0]
#E2_pred = np.array([mdl2.predict(np.array([[t]])) for t in range(0,20)])[:,0]
#
#fig = plt.figure(figsize=(8.75,7))
#plt.plot(E1_pred)
##plt.fill_between(np.arange(0,20), E2_mean-E2_std, E2_mean+E2_std,alpha=0.3)
#plt.xticks(np.arange(0,20),np.arange(-10,10))
#plt.axvline(x=10,c='r',ls='--')
#plt.xlabel('T')
#plt.ylabel('IIC Ratio')
#fig.savefig('E1_pred_snapshot.png')
#
#fig = plt.figure(figsize=(8.75,7))
#plt.plot(E2_pred)
##plt.fill_between(np.arange(0,20), E2_mean-E2_std, E2_mean+E2_std,alpha=0.3)
#plt.xticks(np.arange(0,20),np.arange(-10,10))
#plt.axvline(x=10,c='r',ls='--')
#plt.xlabel('T')
#plt.ylabel('Mean Discharge Freq')
#fig.savefig('E2_pred_snapshot.png')


##outcome regression
#file = open('Outcome_Regression_%d_%d'%(window,times),'w') 
#
#
#rf_outcome = ensemble.RandomForestRegressor()
#rf_outcome = rf_outcome.fit(C,Y_val)
#rf_score = rf_outcome.score(C,Y_val)
#print('Random Forest',file=file)
#print(rf_score,file=file)
#
#ada_outcome = ensemble.AdaBoostRegressor()
#ada_outcome = ada_outcome.fit(C,Y_val)
#ada_score = ada_outcome.score(C,Y_val)
#print('Ada Boost',file=file)
#print(ada_score,file=file)
#
#ridge_outcome = linearmodel.Ridge()
#ridge_outcome = ridge_outcome.fit(C,Y_val)
#ridge_score = ridge_outcome.score(C,Y_val)
#print('Ridge Regression',file=file)
#print(ridge_score,file=file)
#
#svm_outcome = svm.SVR()
#svm_outcome = svm_outcome.fit(C,Y_val)
#svm_score = svm_outcome.score(C,Y_val)
#print('SVR',file=file)
#print(svm_score,file=file)
#
#fig = plt.figure(figsize=(8.75,7))
#feature_imp = rf_outcome.feature_importances_
#y_pos = np.arange(len(Cname))
#plt.bar(y_pos, feature_imp, align='center', alpha=0.5)
#plt.plot(feature_imp)
#plt.xticks(y_pos, Cname, rotation=90)
#plt.ylabel('Feature Importance')
#plt.tight_layout()
#fig.savefig('rf_featureimportance_%d_%d.png'%(window,times))
#
#fig = plt.figure(figsize=(9.75,8))
#coef = ridge_outcome.coef_
#y_pos = np.arange(len(Cname))
#plt.bar(y_pos, coef, align='center', alpha=0.5)
#plt.plot(coef)
#plt.xticks(y_pos, Cname, rotation=90)
#plt.ylabel('Coefficient')
#plt.tight_layout()
#fig.savefig('ridge_coef_%d_%d.png'%(window,times))
#
#file.close()
#



##off_policy
torch.manual_seed(0)
np.random.seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

def reward_mgh(Eit,Dit):
    return 0
#    state_fn = -0.25*( Eit[-3]+Eit[-2]+Eit[-1] )
#    action_fn = -1 * np.e**np.sum(Dit)
#    if Dit.any() < 0:
#         action_fn = -np.inf + action_fn
#    return  state_fn + action_fn
    
lag = 8
forecast=10
Ename = featureprime['Ename']
Dname = featureprime['Dname']
QL_off_policy = QLearning(state_space_dimension=7,action_space_dimension=2,covariate_space_dimension=61, off_policy=True, reward = reward_mgh, discount=1, model='random-forest')
QL_off_policy = QL_off_policy.fit(featureprime,verbose=False,itr=1)
QL_off_policy = QL_off_policy.fit_simulator(featureprime, lag=lag, forecast=forecast, verbose=False)


fe_sim_E = QL_off_policy.S.feature_importances_[:2*lag]
label_sim_e = []
for i in range(0,lag):
    for j in range(0,len(Ename)):
        label_sim_e.append(Ename[j]+str([-(lag-i)]))
        
fig = plt.figure(figsize=(8.75,7))
y_pos = np.arange(len(label_sim_e))
plt.bar(y_pos, fe_sim_E, align='center', alpha=0.5)
plt.plot(fe_sim_E)
plt.xticks(y_pos, label_sim_e, rotation=90)
plt.ylabel('Feature Importance')
plt.tight_layout()
fig.savefig('rf_featureimportance_E_simulator_%d_%d_%d.png'%(window,times,lag))

fe_sim_D = QL_off_policy.S.feature_importances_[7*lag:9*lag]
label_sim_d = []
for i in range(0,lag):
    for j in range(0,len(Dname)):
        label_sim_d.append(Dname[j]+str([-(lag-i)]))
        
fig = plt.figure(figsize=(8.75,7))
y_pos = np.arange(len(label_sim_d))
plt.bar(y_pos, fe_sim_D, align='center', alpha=0.5)
plt.plot(fe_sim_D)
plt.xticks(y_pos, label_sim_d, rotation=90)
plt.ylabel('Feature Importance')
plt.tight_layout()
fig.savefig('rf_featureimportance_D_simulator_%d_%d_%d.png'%(window,times,lag))

fe_sim_C = QL_off_policy.S.feature_importances_[4*lag:]
label_sim_c = []
for j in range(0,len(Cname)):
    label_sim_c.append(Cname[j])
        
for case in range(0,1):
    init_features = {}
    init_features['E'] = E[case][:QL_off_policy.lag,:]
    init_features['D'] = D[case][:QL_off_policy.lag-1,:]
    init_features['C'] = C[case]
    Esim, Dsim = QL_off_policy.run_simulator(init_features,T=100)

plt.plot(np.arange(0,len(E[0])),E[0][:,0],'g--', linewidth=2, markersize=12)
for i in range(0,len(E[0])-QL_off_policy.lag):
    E0 = E[0][i:i+QL_off_policy.lag,:]
    D0 = D[0][i:i+QL_off_policy.lag,:]
    ex, dx, cx = E0.reshape(1,-1)[0], D0.reshape(1,-1)[0], C[0]
    X0t = np.hstack((ex,dx,cx))
    E1 = QL_off_policy.S.predict(X0t.reshape(1,-1))
    E1 = E1.reshape((QL_off_policy.forecast,2))
    plt.plot(np.arange(i+QL_off_policy.lag,i+QL_off_policy.lag+QL_off_policy.forecast),E1[:,0])



file = open('Q_off_policy_outcome_%d_%d.csv'%(window,times),'w') 
Q_off_policy_last = []
for i in range(0,len(E)):
    vec = np.hstack( (E[i][-1],D[i][-1],C[i]) )
    Q_off_policy_last.append( QL_off_policy.Q.predict( np.array([vec]) ) [0] )
    print( QL_off_policy.Q.predict( np.array([vec]) ),Y_val[i],Y[i], sep=', ',file=file)
file.close()

#on_policy
torch.manual_seed(0)
np.random.seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

QL_on_policy = QLearning(state_space_dimension=17,action_space_dimension=3,covariate_space_dimension=61, off_policy=False, reward = reward_mgh, discount=1, model='ridge')
QL_on_policy = QL_on_policy.fit(featureprime,verbose=False,itr=1)

file = open('Q_on_policy_outcome_%d_%d.csv'%(window,times),'w') 
Q_on_policy_last = []
for i in range(0,len(E)):
    vec = np.hstack( (E[i][-1],D[i][-1],C[i]) )
    Q_on_policy_last.append( QL_on_policy.Q.predict( np.array([vec]) ) [0] )
    print( QL_on_policy.Q.predict( np.array([vec]) ),Y_val[i],Y[i], sep=', ',file=file)
file.close()

#making boxplots
def reject_outliers(data, m=1):
    return data[abs(data - np.mean(data)) < m * np.std(data)]

uniques = list(np.unique(Y))

#
d_on_policy = [ [] for i in range(0,len(uniques))]
for i in range(0,len(Y)):
    d_on_policy[ uniques.index(Y[i]) ] = d_on_policy[ uniques.index(Y[i]) ] + [Q_on_policy_last[i]]

d_on_policy = [ reject_outliers(np.array(d_on_policy[i])) for i in range(0,len(uniques))]

#
d_off_policy = [ [] for i in range(0,len(uniques))]
for i in range(0,len(Y)):
    d_off_policy[ uniques.index(Y[i]) ] = d_off_policy[ uniques.index(Y[i]) ] + [Q_off_policy_last[i]]

d_off_policy = [ reject_outliers(np.array(d_off_policy[i])) for i in range(0,len(uniques))]


fig = plt.figure(figsize=(80.75,70))
plt.rcParams.update({'font.size': 28.5})
blue = mpatches.Patch(color='#1f77b4',alpha = 0.65)
bplot = plt.boxplot(d_on_policy,showmeans=False,patch_artist=blue)
for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:
    plt.setp(bplot[element], color='#1f77b4')
for patch in bplot['boxes']:
    patch.set_facecolor('#1f77b4') 
    patch.set_alpha(0.2)                 
orange = mpatches.Patch(color='#ff7f0e',alpha = 0.65)
bplot = plt.boxplot(d_off_policy,showmeans=False,patch_artist=orange)
for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:
    plt.setp(bplot[element], color='#ff7f0e')
for patch in bplot['boxes']:
    patch.set_facecolor('#ff7f0e')
    patch.set_alpha(0.2) 
plt.legend([blue,orange],['On-Policy','Off-Policy'])#,loc='lower center', bbox_to_anchor=(0.5, 1.05),ncol=2)
plt.xticks([1,2,3,4,5,6,7],uniques,rotation=15)
plt.ylabel('Q-Value')
plt.xlabel('Actual Outcome')
#plt.ylim((-50,10))
plt.tight_layout()
plt.show()
fig.savefig('mgh_policy_%d_%d.png'%(window,times))


#fig = plt.figure(figsize=(8.75,7))
#feature_imp = QL_off_policy.Q.feature_importances_[:17]
#y_pos = np.arange(len(QL_off_policy.Q.feature_importances_[:17]))
#plt.bar(y_pos, feature_imp, align='center', alpha=0.5)
#plt.plot(feature_imp)
#plt.xticks(y_pos, list(map(str,Ename)), rotation=90)
#plt.ylabel('Feature Importance')
#plt.tight_layout()
#fig.savefig('QL_off_featureimportance_%d_%d.png'%(window,times))

'''
#----------------------------
###DUMP DUMP DUMP
#----------------------------
#fig = plt.figure(figsize=(8.75,7))
#plt.imshow(Di.T, cmap='hot', interpolation='none')
#plt.yticks(np.arange(0,3), Dname)
#plt.tight_layout()
#plt.colorbar()
#fig.savefig('mgh_sim_drug_on_policy_case_%d_%d.png'%(case,window))
#
#fig = plt.figure(figsize=(8.75,7))
#plt.imshow(A_array.T, cmap='hot', interpolation='none')
#plt.yticks(np.arange(0,3), Dname)
#plt.tight_layout()
#plt.colorbar()
#fig.savefig('mgh_drug_off_policy_case_%d_%d.png'%(case,window))
#
#fig = plt.figure(figsize=(8.75,7))
#plt.imshow(A_array_sim.T, cmap='hot', interpolation='none')
#plt.yticks(np.arange(0,3), Dname)
#plt.tight_layout()
#plt.colorbar()
#fig.savefig('mgh_sim_drug_off_policy_case_%d_%d.png'%(case,window))


#action_dim = 34
#state_dim = 5
#log = open('log_mgh.log','w')
## Linear model Q(s,a) =  w[1,s,a]
#class LinearQ:
#    def __init__(self,state_dim,action_dim):
#        self.w = np.random.normal(0,5,1 + state_dim + action_dim )
#
#    def lin_Q(self,s,a):
#        vec = np.hstack(([1],s,a))
#        return np.dot(self.w,vec)
#
#    def loss(self,s,a,qt):
#        q = self.lin_Q(s,a)
#        return ( qt - q )**2
#    
#    def lin_gradient_loss(self,s,a,qt):
#        vec = np.hstack(([1],s,a))
#        q = self.lin_Q(s,a)
#        grad_w = -2*( qt - q )*vec
##        if not np.isnan(grad_w).all():
##            print( np.average(grad_w), self.loss(s,a,qt)  )
#        return grad_w
#    
#    def update_weights(self,grad_w,alpha=0.05):
#        self.w = self.w - alpha * grad_w
#        return self.w
#        
#    def fit(self,S,A,Y,R=None,gamma=1,alpha=0.05,iterations=10):
#        n = len(S)
#        for itr in range(0,iterations):
#            for i in range(0,n):
#                states = S[i][~np.isnan(S[i]).any(axis=1)]
#                actions = A[i][~np.isnan(S[i]).any(axis=1)]
#                T,ds = states.shape
#                _,da = actions.shape
#                reward = np.zeros((T,))
#                reward[T-1] = Y[i]
#                for t in range(T-1,-1,-1):
#                    if t<(T-1):
#                        qt = reward[t] + gamma*self.lin_Q(states[t+1,:],actions[t+1,:])
#                    elif t==(T-1):
#                        qt = reward[t]
#                    grad_w = self.lin_gradient_loss(states[t],actions[t],qt)
#                    self.w = self.update_weights(grad_w,alpha=alpha)
#        return self
#
#linQ = LinearQ( state_dim = state_dim, action_dim = action_dim )
#linQfitted = linQ.fit( S=data['E'], A=data['D'], Y=-data['Y'], alpha=0.0001 )
#
##def define_model(input_size,output_size):
##  model = Sequential()
##  model.add( Dense(units=20, input_shape=(input_size,) ) )
##  model.add( Activation('relu') )
##  model.add( Dense(output_size) )
##  model.add( Activation('linear') )
##  print( model.summary() )
##  return model
##
##model = define_model(1,nb_actions)
##
##"""Fitting the model"""
##
##policy = EpsGreedyQPolicy()
##memory = SequentialMemory(limit=50000, window_length=1)
##dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,
##target_model_update=1e-2, policy=policy)
##dqn.compile(Adam(lr=1e-3), metrics=['mae'])
##
##
##dqn.fit(env, nb_steps=5000, visualize=False, verbose=1)
##
##dqn.test(env, nb_episodes=5, visualize=False)


#"""# Naive Data Generation"""

#class StupidDrugRegime(gym.Env):
#  def __init__(self):
#      self.range = 1000  # +/- value the randomly select number can be between
#      self.bounds = 10  # Action space bounds
#
#      self.action_space = spaces.Discrete(self.bounds)
#      self.observation_space = spaces.Discrete(4)
#
#      self.number = 0
#      self.guess_count = 0
#      self.guess_max = 200
#      self.observation = 0
#
#      self.seed()
#      self.reset()
#
#  def seed(self, seed=None):
#      self.np_random, seed = seeding.np_random(seed)
#      return [seed]
#
#  def step(self, action):
#      assert self.action_space.contains(action)
#
#      if action < self.number:
#          self.observation = 3
#
#      elif action == self.number:
#          self.observation = 2
#
#      elif action > self.number:
#          self.observation = 1
#
#      reward = ((min(action, self.number) + self.bounds) / (max(action, self.number) + self.bounds)) ** 2
#
#      self.guess_count += 1
#      done = self.guess_count >= self.guess_max
#
#      return self.observation, reward, done, {"number": self.number, "guesses": self.guess_count}
#
#  def reset(self):
#      self.number = self.np_random.randint(0,self.bounds)
#      self.guess_count = 0
#      self.observation = 0
#      return self.observation
#
#
#"""Setting up the environment"""
#
#env = StupidDrugRegime()
#np.random.seed(123)
#env.seed(123)
#nb_actions = env.action_space.n




#class Neural_Network(nn.Module):
#    def __init__(self,inputSize,outputSize,hiddenSize=64):
#        super(Neural_Network, self).__init__()
#        # parameters
#        # TODO: parameters can be parameterized instead of declaring them here
#        self.inputSize = inputSize
#        self.outputSize = outputSize
#        self.hiddenSize = hiddenSize
#        
#        # weights
#        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor
#        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor
#        
#    def forward(self, X):
#        self.z = torch.matmul(X, self.W1) # 3 X 3 ".dot" does not broadcast in PyTorch
#        self.z2 = self.sigmoid(self.z) # activation function
#        self.z3 = torch.matmul(self.z2, self.W2)
#        o = self.sigmoid(self.z3) # final activation function
#        return o
#        
#    def sigmoid(self, s):
#        return 1 / (1 + torch.exp(-s))
#    
#    def sigmoidPrime(self, s):
#        # derivative of sigmoid
#        return s * (1 - s)
#    
#    def backward(self, X, o, error ):
#        self.o_error = error # error in output
#        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error
#        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))
#        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)
#        self.W1 += torch.matmul(torch.t(X), self.z2_delta)
#        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)
#        
#    def train(self, X, y):
#        # forward + backward pass for training
#        o = self.forward(X)
#        error = y - o
#        self.backward(X, o, error )
#        
#    def saveWeights(self, model):
#        # we will use the PyTorch internal storage functions
#        torch.save(model, "NN")
#        # you can reload model with all the weights and so forth with:
#        # torch.load("NN")
#        
##    def predict(self):
##        print ("Predicted data based on trained weights: ")
##        print ("Input (scaled): \n" + str(xPredicted))
##        print ("Output: \n" + str(self.forward(xPredicted)))
        
        
#    def fit( self, features, learningrate=0.05, losscutoff=50000, verbose=False, itr=100):
#        Y = features['Y']
#        C = features['C']
#        E = features['E']
#        D = features['D']
#        n = len(Y)
#        for j in range(0,itr):
#            for i in range(0,n):
#                #first learn last step
#                yi = Y[i]
#                ci = C[i]
#                Ei = E[i]
#                Di = D[i]
#                T,sd = Ei.shape
#                _,ad = Di.shape
#                t = T-1
#                vec_t = torch.tensor([ np.hstack( (Ei[t],Di[t],ci) ) ],dtype=torch.float)
#                val_t1 = torch.tensor([[-1*10**yi + 1]],dtype=torch.float)
##                if yi == 0 or yi == 1 or yi == 2:
##                    val_t1 = torch.tensor([[ 0.0 ]],dtype=torch.float)
##                elif yi == 3:
##                    val_t1 = torch.tensor([[ 0.0 ]],dtype=torch.float)
##                else:
##                    val_t1 = torch.tensor([[ -10000.0 ]],dtype=torch.float)
#                returnval = self.Q.fit(x=vec_t,y=val_t1,learningrate=learningrate,losscutoff=losscutoff,verbose=verbose)
#                if returnval<0:
#                    learningrate = learningrate/2
#        for itr in range(itr*n):
#            #learn random steps
#            i = np.random.randint(0,n)
#            yi = Y[i]
#            ci = C[i]
#            Ei = E[i]
#            Di = D[i]
#            T,sd = Ei.shape
#            _,ad = Di.shape
#            t = np.random.randint(0,T)
#            vec_t = torch.tensor([ np.hstack( (Ei[t],Di[t],ci) ) ],dtype=torch.float)
#            if (t+1)<T:
#                vec1_t1 = torch.tensor([ np.hstack( (Ei[t+1],Di[t],ci) ) ],dtype=torch.float)
#                if self.off_policy:
#                    Di1 = self.A(vec1_t1)
#                    vec2_t1 = torch.cat( [ torch.tensor( Ei[t+1], dtype=torch.float ), Di1[0], torch.tensor(ci,dtype=torch.float) ])
#                    q_t1 = self.Q(vec2_t1)
#                    if verbose:
#                        print('fitting action function')
#                    self.A.fit(x=vec1_t1,loss=-q_t1,learningrate=learningrate,losscutoff=losscutoff,verbose=verbose)
#                else:
#                    Di1 = torch.tensor([ Di[t+1]], dtype=torch.float )
#                    vec2_t1 = torch.cat( [ torch.tensor( Ei[t+1], dtype=torch.float ), Di1[0], torch.tensor(ci,dtype=torch.float) ])
#                    q_t1 = self.Q(vec2_t1)
#                    if verbose:
#                        print('fitting action function')
#                    self.A.fit(x=vec1_t1,y=Di1,learningrate=learningrate,losscutoff=losscutoff,verbose=verbose)
#                self.S.fit(x=vec_t,y=torch.tensor([Ei[t+1]], dtype=torch.float ),learningrate=learningrate,losscutoff=losscutoff,verbose=verbose)
#                val_t1 = torch.tensor(self.reward(Ei[t],Di[t]),dtype=torch.float) + torch.tensor(self.discount,dtype=torch.float) * q_t1
#            else:
#                val_t1 = torch.tensor([[-1*10**yi + 1]],dtype=torch.float)
##                if yi == 0 or yi == 1 or yi == 2:
##                    val_t1 = torch.tensor([[ 0.0 ]],dtype=torch.float)
###                    val_t1 = torch.tensor([[ 100000/(yi+1) ]],dtype=torch.float)
##                elif yi == 3:
##                    val_t1 = torch.tensor([[ 0.0 ]],dtype=torch.float)
##                else:
##                    val_t1 = torch.tensor([[ -10000.0 ]],dtype=torch.float)
#            if verbose:
#                print('fitting Q function')
#            returnval = self.Q.fit(x=vec_t,y=val_t1,learningrate=learningrate,losscutoff=losscutoff,verbose=verbose)
#            if returnval<0:
#                learningrate = learningrate/2
#        return self
'''