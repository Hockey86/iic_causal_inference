# -*- coding: utf-8 -*-
"""MGH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1328EbQmeZ9maTWn5AGkikTTttcHecddR

# Setting up
"""

import sys
import os
#!{sys.executable} -m pip install keras-rl

import numpy as np
import pickle

import neuralnet 

import torch
import torch.nn as nn

import matplotlib.pyplot as plt

#import gym
#from gym import spaces
#from gym.utils import seeding
#
#from keras.models import Sequential
#from keras.layers import Dense, Activation, Flatten
#from keras.optimizers import Adam
#
#from rl.agents.dqn import DQNAgent
#from rl.policy import EpsGreedyQPolicy
#from rl.memory import SequentialMemory

#import pickle
#
#from sklearn.ensemble import RandomForestRegressor as RFR
#from sklearn.svm import SVR
#
#import sklearn.linear_model as lm
#
#import sklearn.neural_network as nn

#import torch
#import torch.nn as nn

datafile = open('../IICdata/preprocessed_dataset_all.pickle','rb')
data = pickle.load( datafile , encoding='latin')

big_datafile = open('../IICdata/for_Harsh/data/big_dataset.pickle','rb')
big_dataset = pickle.load( big_datafile , encoding='latin')

featurefile = open('../IICdata/for_Harsh/data/features_window_600.pickle','rb')
features = pickle.load( featurefile , encoding='latin')



"""# Q-Learning
Defining the model
"""



class QLearning:
    def __init__( self, state_space_dimension, action_space_dimension, covariate_space_dimension, off_policy=True ):
        self.state_space_dimension = state_space_dimension
        self.action_space_dimension = action_space_dimension
        self.covariate_space_dimension = covariate_space_dimension
        self.Q = neuralnet.Net(self.state_space_dimension+self.action_space_dimension+self.covariate_space_dimension,1,[200])
        self.A = neuralnet.Net(self.state_space_dimension+self.covariate_space_dimension,self.action_space_dimension,[300])
        self.off_policy = off_policy
    
    def fit( self, features, learningrate=0.01, losscutoff=50000, verbose=False):
        Y = features['Y']
        C = features['C']
        E = features['E']
        D = features['D']
        n = len(Y)
        for j in range(0,100):
            for i in range(0,n):
                #first learn last step
                yi = Y[i]
                ci = C[i]
                Ei = E[i]
                Di = D[i]
                T,sd = Ei.shape
                _,ad = Di.shape
                t = T-1
                vec_t = torch.tensor([ np.hstack( (Ei[t],Di[t],ci) ) ],dtype=torch.float)
                if yi == 0 or yi == 1 or yi == 2:
                    val_t1 = torch.tensor([[ 10000/(yi+1) ]],dtype=torch.float)
                elif yi == 3:
                    val_t1 = torch.tensor([[ 0 ]],dtype=torch.float)
                else:
                    val_t1 = torch.tensor([[ -2500*yi ]],dtype=torch.float)
                self.Q.fit(x=vec_t,y=val_t1,learningrate=learningrate,losscutoff=losscutoff,verbose=verbose)
        for itr in range(50*n):
            #learn random steps
            i = np.random.randint(0,n)
            yi = Y[i]
            ci = C[i]
            Ei = E[i]
            Di = D[i]
            T,sd = Ei.shape
            _,ad = Di.shape
            t = np.random.randint(0,T)
            vec_t = torch.tensor([ np.hstack( (Ei[t],Di[t],ci) ) ],dtype=torch.float)
            if (t+1)<T and itr>100:
                vec1_t1 = torch.tensor([ np.hstack( (Ei[t+1],ci) ) ],dtype=torch.float)
                if self.off_policy:
                    Di1 = self.A(vec1_t1)
                else:
                    Di1 = torch.tensor([ Di[t+1]], dtype=torch.float )
                vec2_t1 = torch.cat( [ torch.tensor( Ei[t+1], dtype=torch.float ), Di1[0], torch.tensor(ci,dtype=torch.float) ])
                #torch.tensor([ np.hstack( (Ei[t+1],Di1,ci) ) ],dtype=torch.float)
                val_t1 = self.Q(vec2_t1)
                if verbose:
                    print('fitting action function')
                self.A.fit(x=vec1_t1,loss=-val_t1,learningrate=learningrate,losscutoff=losscutoff,verbose=verbose)
            else:
                if yi == 0 or yi == 1 or yi == 2:
                    val_t1 = torch.tensor([[ 10000/(yi+1) ]],dtype=torch.float)
                elif yi == 3:
                    val_t1 = torch.tensor([[ 0 ]],dtype=torch.float)
                else:
                    val_t1 = torch.tensor([[ -2500*yi ]],dtype=torch.float)
            if verbose:
                print('fitting Q function')
            self.Q.fit(x=vec_t,y=val_t1,learningrate=learningrate,losscutoff=losscutoff,verbose=verbose)
        return self
    

Y = features['Y']
C = features['C']
E = features['E']
D = features['D']

#off_policy
QL_off_policy = QLearning(state_space_dimension=17,action_space_dimension=34,covariate_space_dimension=61, off_policy=True )
QL_off_policy = QL_off_policy.fit(features,learningrate=0.0000005,losscutoff=101296,verbose=True)

file = open('Q_off_policy_outcome.csv','w') 
Q_off_policy_last = []
for i in range(0,1308):
    Q_off_policy_last.append( float(QL_off_policy.Q(torch.tensor([ np.hstack( (E[i][-1],D[i][-1],C[i]) ) ],dtype=torch.float) )[0][0] ) )
    print(QL_off_policy.Q(torch.tensor([ np.hstack( (E[i][-1],D[i][-1],C[i]) ) ],dtype=torch.float) )[0],Y[i],sep=', ',file=file)
file.close()

#on_policy
QL_on_policy = QLearning(state_space_dimension=17,action_space_dimension=34,covariate_space_dimension=61, off_policy=False )
QL_on_policy = QL_on_policy.fit(features,learningrate=0.0000005,losscutoff=101296,verbose=True)

file = open('Q_on_policy_outcome.csv','w') 
Q_on_policy_last = []
for i in range(0,1308):
    Q_on_policy_last.append( float(QL_on_policy.Q(torch.tensor([ np.hstack( (E[i][-1],D[i][-1],C[i]) ) ],dtype=torch.float) )[0][0] ) )
    print(QL_on_policy.Q(torch.tensor([ np.hstack( (E[i][-1],D[i][-1],C[i]) ) ],dtype=torch.float) )[0],Y[i],sep=', ',file=file)
file.close()

#making boxplots
def reject_outliers(data, m=2):
    return data[abs(data - np.mean(data)) < m * np.std(data)]

uniques = list(np.unique(Y))

#
d_on_policy = [ [] for i in range(0,len(uniques))]
for i in range(0,len(Y)):
    d_on_policy[ uniques.index(Y[i]) ] = d_on_policy[ uniques.index(Y[i]) ] + [Q_on_policy_last[i]]

d_on_policy = [ reject_outliers(np.array(d_on_policy[i])) for i in range(0,len(uniques))]

#
d_off_policy = [ [] for i in range(0,len(uniques))]
for i in range(0,len(Y)):
    d_off_policy[ uniques.index(Y[i]) ] = d_off_policy[ uniques.index(Y[i]) ] + [Q_off_policy_last[i]]

d_off_policy = [ reject_outliers(np.array(d_off_policy[i])) for i in range(0,len(uniques))]

plt.violinplot(d_on_policy,showmeans=False, showextrema=True, showmedians=True)
plt.violinplot(d_off_policy,showmeans=False, showextrema=True, showmedians=True)
plt.xticks([1,2,3,4,5,6,7],uniques,rotation=15)


#action_dim = 34
#state_dim = 5
#log = open('log_mgh.log','w')
## Linear model Q(s,a) =  w[1,s,a]
#class LinearQ:
#    def __init__(self,state_dim,action_dim):
#        self.w = np.random.normal(0,5,1 + state_dim + action_dim )
#
#    def lin_Q(self,s,a):
#        vec = np.hstack(([1],s,a))
#        return np.dot(self.w,vec)
#
#    def loss(self,s,a,qt):
#        q = self.lin_Q(s,a)
#        return ( qt - q )**2
#    
#    def lin_gradient_loss(self,s,a,qt):
#        vec = np.hstack(([1],s,a))
#        q = self.lin_Q(s,a)
#        grad_w = -2*( qt - q )*vec
##        if not np.isnan(grad_w).all():
##            print( np.average(grad_w), self.loss(s,a,qt)  )
#        return grad_w
#    
#    def update_weights(self,grad_w,alpha=0.05):
#        self.w = self.w - alpha * grad_w
#        return self.w
#        
#    def fit(self,S,A,Y,R=None,gamma=1,alpha=0.05,iterations=10):
#        n = len(S)
#        for itr in range(0,iterations):
#            for i in range(0,n):
#                states = S[i][~np.isnan(S[i]).any(axis=1)]
#                actions = A[i][~np.isnan(S[i]).any(axis=1)]
#                T,ds = states.shape
#                _,da = actions.shape
#                reward = np.zeros((T,))
#                reward[T-1] = Y[i]
#                for t in range(T-1,-1,-1):
#                    if t<(T-1):
#                        qt = reward[t] + gamma*self.lin_Q(states[t+1,:],actions[t+1,:])
#                    elif t==(T-1):
#                        qt = reward[t]
#                    grad_w = self.lin_gradient_loss(states[t],actions[t],qt)
#                    self.w = self.update_weights(grad_w,alpha=alpha)
#        return self
#
#linQ = LinearQ( state_dim = state_dim, action_dim = action_dim )
#linQfitted = linQ.fit( S=data['E'], A=data['D'], Y=-data['Y'], alpha=0.0001 )
#
##def define_model(input_size,output_size):
##  model = Sequential()
##  model.add( Dense(units=20, input_shape=(input_size,) ) )
##  model.add( Activation('relu') )
##  model.add( Dense(output_size) )
##  model.add( Activation('linear') )
##  print( model.summary() )
##  return model
##
##model = define_model(1,nb_actions)
##
##"""Fitting the model"""
##
##policy = EpsGreedyQPolicy()
##memory = SequentialMemory(limit=50000, window_length=1)
##dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,
##target_model_update=1e-2, policy=policy)
##dqn.compile(Adam(lr=1e-3), metrics=['mae'])
##
##
##dqn.fit(env, nb_steps=5000, visualize=False, verbose=1)
##
##dqn.test(env, nb_episodes=5, visualize=False)


"""# Naive Data Generation"""

#class StupidDrugRegime(gym.Env):
#  def __init__(self):
#      self.range = 1000  # +/- value the randomly select number can be between
#      self.bounds = 10  # Action space bounds
#
#      self.action_space = spaces.Discrete(self.bounds)
#      self.observation_space = spaces.Discrete(4)
#
#      self.number = 0
#      self.guess_count = 0
#      self.guess_max = 200
#      self.observation = 0
#
#      self.seed()
#      self.reset()
#
#  def seed(self, seed=None):
#      self.np_random, seed = seeding.np_random(seed)
#      return [seed]
#
#  def step(self, action):
#      assert self.action_space.contains(action)
#
#      if action < self.number:
#          self.observation = 3
#
#      elif action == self.number:
#          self.observation = 2
#
#      elif action > self.number:
#          self.observation = 1
#
#      reward = ((min(action, self.number) + self.bounds) / (max(action, self.number) + self.bounds)) ** 2
#
#      self.guess_count += 1
#      done = self.guess_count >= self.guess_max
#
#      return self.observation, reward, done, {"number": self.number, "guesses": self.guess_count}
#
#  def reset(self):
#      self.number = self.np_random.randint(0,self.bounds)
#      self.guess_count = 0
#      self.observation = 0
#      return self.observation
#
#
#"""Setting up the environment"""
#
#env = StupidDrugRegime()
#np.random.seed(123)
#env.seed(123)
#nb_actions = env.action_space.n




#class Neural_Network(nn.Module):
#    def __init__(self,inputSize,outputSize,hiddenSize=64):
#        super(Neural_Network, self).__init__()
#        # parameters
#        # TODO: parameters can be parameterized instead of declaring them here
#        self.inputSize = inputSize
#        self.outputSize = outputSize
#        self.hiddenSize = hiddenSize
#        
#        # weights
#        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor
#        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor
#        
#    def forward(self, X):
#        self.z = torch.matmul(X, self.W1) # 3 X 3 ".dot" does not broadcast in PyTorch
#        self.z2 = self.sigmoid(self.z) # activation function
#        self.z3 = torch.matmul(self.z2, self.W2)
#        o = self.sigmoid(self.z3) # final activation function
#        return o
#        
#    def sigmoid(self, s):
#        return 1 / (1 + torch.exp(-s))
#    
#    def sigmoidPrime(self, s):
#        # derivative of sigmoid
#        return s * (1 - s)
#    
#    def backward(self, X, o, error ):
#        self.o_error = error # error in output
#        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error
#        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))
#        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)
#        self.W1 += torch.matmul(torch.t(X), self.z2_delta)
#        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)
#        
#    def train(self, X, y):
#        # forward + backward pass for training
#        o = self.forward(X)
#        error = y - o
#        self.backward(X, o, error )
#        
#    def saveWeights(self, model):
#        # we will use the PyTorch internal storage functions
#        torch.save(model, "NN")
#        # you can reload model with all the weights and so forth with:
#        # torch.load("NN")
#        
##    def predict(self):
##        print ("Predicted data based on trained weights: ")
##        print ("Input (scaled): \n" + str(xPredicted))
##        print ("Output: \n" + str(self.forward(xPredicted)))