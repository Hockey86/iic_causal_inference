{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import os\n",
    "import timeit\n",
    "import scipy.io as sio\n",
    "from scipy.special import logit\n",
    "from scipy.special import expit as sigmoid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "import pystan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/data/Dropbox (Partners HealthCare)/CausalModeling_IIIC/data_to_share/step1_output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define PK time constants for each drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 20 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "halflife = pd.DataFrame({\n",
    "    'lacosamide':[13],\n",
    "    'levetiracetam':[6],\n",
    "    'midazolam':[1.5],\n",
    "    'pentobarbital':[15],\n",
    "    'phenobarbital':[53],\n",
    "    'phenytoin':[22],\n",
    "    'propofol':[1.5],\n",
    "    'valproate':[8]\n",
    "    },index=['t1/2'])\n",
    "\n",
    "halflife = halflife.append(np.log(2) / halflife.rename(index={'t1/2':'k'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drug_concentration(d_ts,k):\n",
    "    \"\"\"\n",
    "    d_ts.shape = (#drug, T)\n",
    "    \"\"\"\n",
    "    k_ts = np.array([ np.exp(-k*t) for t in range(d_ts.shape[1]) ]).T\n",
    "    conc = np.array([np.convolve(d_ts[i],k_ts[i],'full') for i in range(d_ts.shape[0])])\n",
    "    conc = conc[:,:d_ts.shape[1]]\n",
    "    return conc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsigmoid(x):\n",
    "    \"\"\"\n",
    "    Computes the log(sigmoid(x))\n",
    "    http://fa.bianp.net/blog/2019/evaluate_logistic/#sec2\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    out = np.zeros_like(x)\n",
    "    idx0 = x < -33\n",
    "    out[idx0] = x[idx0]\n",
    "    idx1 = (x >= -33) & (x < -18)\n",
    "    out[idx1] = x[idx1] - np.exp(x[idx1])\n",
    "    idx2 = (x >= -18) & (x < 37)\n",
    "    out[idx2] = -np.log1p(np.exp(-x[idx2]))\n",
    "    idx3 = x >= 37\n",
    "    out[idx3] = -np.exp(-x[idx3])\n",
    "    if type(x)==float or x.ndim==0:\n",
    "        out = float(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patient(file):\n",
    "    window = 900\n",
    "    step   = 900\n",
    "    \n",
    "    #if '.mat' in file:\n",
    "    s = sio.loadmat(os.path.join(DATA_DIR, file))\n",
    "    human_iic = s['human_iic'][0].astype(float)\n",
    "    spike = s['spike'][0].astype(float)\n",
    "    drugs = s['drugs_weightnormalized'].astype(float)\n",
    "    artifact = s['artifact'][0].astype(float)\n",
    "    human_iic[artifact==1] = np.nan\n",
    "    spike[artifact==1] = np.nan\n",
    "\n",
    "    drugnames = list(map(lambda x: x.strip(), s['Dnames']))\n",
    "    drugs_window = np.array([ np.mean(drugs[i:i+window],axis=0) for i in range(0,len(drugs),step) ])\n",
    "\n",
    "    sz_burden = (human_iic==1).astype(float)\n",
    "    sz_burden[np.isnan(human_iic)] = np.nan\n",
    "    sz_burden_window = [np.nanmean(sz_burden[i:i+window]) for i in range(0, len(sz_burden),step)]\n",
    "\n",
    "    iic_burden = np.in1d(human_iic, [1,2,3,4]).astype(float)\n",
    "    iic_burden[np.isnan(human_iic)] = np.nan\n",
    "    iic_burden_window = [np.nanmean(iic_burden[i:i+window]) for i in range(0, len(iic_burden),step)]\n",
    "\n",
    "    spike_rate_window = [np.nanmean(spike[i:i+window]) for i in range(0, len(spike),step)]\n",
    "\n",
    "    df = pd.DataFrame(data=np.c_[sz_burden_window, iic_burden_window, spike_rate_window, drugs_window],\n",
    "                      columns=['sz_burden', 'iic_burden', 'spike_rate']+drugnames)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sid):  # previsously called patient_data\n",
    "    PK_K = halflife.loc['k'].to_numpy()\n",
    "\n",
    "    #fetch the data\n",
    "    file = sid + '.mat'\n",
    "    p = patient(file)\n",
    "\n",
    "    #setting up the data\n",
    "    response_tostudy = ['iic_burden']\n",
    "    Eobs = p[response_tostudy].values.flatten()\n",
    "\n",
    "    #PK\n",
    "    drugs_tostudy = ['lacosamide', 'levetiracetam', 'midazolam', \n",
    "                    'pentobarbital','phenobarbital', 'phenytoin',\n",
    "                    'propofol', 'valproate']\n",
    "    Ddose = p[drugs_tostudy].fillna(0).to_numpy().T\n",
    "    D = drug_concentration(Ddose, PK_K).T\n",
    "\n",
    "    cov_tostudy = ['Age']\n",
    "    C = pd.read_csv(os.path.join(DATA_DIR, 'covariates.csv'))\n",
    "    C = C[C.Index==sid][cov_tostudy].iloc[0]\n",
    "    \n",
    "    #Eobs.shape = (T,)\n",
    "    #D.shape = (T,#drug)\n",
    "    #C.shape = (#covaraites,)\n",
    "    return Eobs, D, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/82 [00:00<?, ?it/s]/home/sunhaoqi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: Mean of empty slice\n",
      "/home/sunhaoqi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:23: RuntimeWarning: Mean of empty slice\n",
      "/home/sunhaoqi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: Mean of empty slice\n",
      "100%|██████████| 82/82 [00:05<00:00, 14.00it/s]\n"
     ]
    }
   ],
   "source": [
    "Pobs = []\n",
    "Eobs = []\n",
    "D = []\n",
    "C = []\n",
    "sids = ['sid2', 'sid8', 'sid13', 'sid17', 'sid18', 'sid30', 'sid36', 'sid39', 'sid54',\n",
    "        'sid56', 'sid69', 'sid77', 'sid82', 'sid88', 'sid91', 'sid92', 'sid297', 'sid327',\n",
    "        'sid385', 'sid395', 'sid400', 'sid403', 'sid406', 'sid424', 'sid450', 'sid456',\n",
    "        'sid490', 'sid512', 'sid551', 'sid557', 'sid734', 'sid736', 'sid801', 'sid821',\n",
    "        'sid824', 'sid827', 'sid832', 'sid833', 'sid834', 'sid839', 'sid848', 'sid849',\n",
    "        'sid852', 'sid872', 'sid876', 'sid880', 'sid881', 'sid884', 'sid886',\n",
    "        'sid915', 'sid940', 'sid942', 'sid944', 'sid952', 'sid960', 'sid965', 'sid967',\n",
    "        'sid983', 'sid987', 'sid988', 'sid994', 'sid1002', 'sid1006', 'sid1016', 'sid1022',\n",
    "        'sid1025', 'sid1034', 'sid1038', 'sid1039', 'sid1055', 'sid1056', 'sid1063', 'sid1113',\n",
    "        'sid1116', 'sid1337', 'sid1913', 'sid1915', 'sid1916', 'sid1917', 'sid1928', 'sid1956', 'sid1966']\n",
    "\n",
    "# exclude sid887 because there is no overlap between drug and IIC\n",
    "# , 'sid887'\n",
    "\n",
    "W=900\n",
    "for sid in tqdm(sids):\n",
    "    Pobs_, D_, C_ = preprocess(sid)\n",
    "    Pobs.append(Pobs_)\n",
    "    Eobs_ = Pobs_*W\n",
    "    Eobs_[np.isnan(Eobs_)] = -1   # convert NaN to -1 for int dtype\n",
    "    Eobs.append(np.round(Eobs_).astype(int))\n",
    "    D.append(D_)\n",
    "    C.append(C_)\n",
    "\n",
    "C = np.array(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print stats of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.         1.         1.         1.         1.\n",
      " 1.         0.91460674 0.63932584 0.         0.24095023 0.31897555\n",
      " 0.40289505 0.64293785 1.         1.         0.73485714 0.\n",
      " 0.         0.         0.         0.         0.         0.21082949\n",
      " 0.         0.1031653  0.04383886 0.06352941 0.14355828 0.3091133\n",
      " 0.83599089 0.85357143 0.78571429 0.79453682 0.48943662 0.18311534\n",
      " 0.22319202 0.         0.70755886 1.         1.                nan\n",
      "        nan        nan        nan 0.83587786 0.81573034 0.95333333\n",
      " 1.         0.98       0.65427928 0.44457275 0.22696629 0.08998875\n",
      " 0.19775281 0.75111111 0.99444444 1.         1.         0.81336406\n",
      " 0.71766029 0.72517321 0.69850403 0.5046729  0.80941447 0.20337079\n",
      " 0.        ]\n",
      "[900 900 900 900 900 900 900 823 575   0 217 287 363 579 900 900 661   0\n",
      "   0   0   0   0   0 190   0  93  39  57 129 278 752 768 707 715 440 165\n",
      " 201   0 637 900 900  -1  -1  -1  -1 752 734 858 900 882 589 400 204  81\n",
      " 178 676 895 900 900 732 646 653 629 454 728 183   0]\n",
      "(82, 1)\n",
      "[49, 50, 50, 50, 50, 51, 51, 51, 52, 52, 52, 52, 52, 53, 53, 53, 53, 55, 56, 56, 56, 56, 56, 56, 57, 57, 58, 58, 58, 59, 60, 60, 61, 61, 61, 61, 61, 61, 62, 62, 62, 63, 63, 64, 64, 64, 65, 66, 66, 67, 67, 68, 68, 68, 68, 69, 70, 72, 72, 75, 75, 78, 80, 83, 83, 87, 91, 101, 103, 105, 110, 121, 139, 153, 181, 195, 245, 247, 334, 453, 657, 1506]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in tqdm(range(len(sids))):\\n    plt.close()\\n    plt.plot(Pobs[i])\\n    plt.title(sids[i])\\n    plt.savefig('E_figures/%s.png'%sids[i])\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plt.plot(E[0])\n",
    "print(Pobs[0])\n",
    "print(Eobs[0])\n",
    "print(C.shape)\n",
    "print(sorted([len(x) for x in D]))\n",
    "\"\"\"\n",
    "for i in tqdm(range(len(sids))):\n",
    "    plt.close()\n",
    "    plt.plot(Pobs[i])\n",
    "    plt.title(sids[i])\n",
    "    plt.savefig('E_figures/%s.png'%sids[i])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove long gaps in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 29, 34, 37, 38, 39, 48, 49, 50, 50, 50, 50, 51, 51, 51, 51, 52, 52, 52, 52, 52, 53, 53, 53, 53, 54, 55, 55, 56, 56, 56, 56, 56, 56, 57, 57, 58, 58, 58, 58, 59, 60, 60, 61, 61, 61, 61, 61, 61, 62, 62, 62, 63, 63, 63, 64, 64, 64, 65, 65, 66, 66, 67, 67, 67, 68, 68, 68, 68, 69, 70, 72, 72, 75, 75, 78, 80, 83, 83, 91, 103, 105]\n"
     ]
    }
   ],
   "source": [
    "ind = sids.index('sid1038') # T=1506\n",
    "Pobs[ind] = Pobs[ind][1469:]\n",
    "Eobs[ind] = Eobs[ind][1469:]\n",
    "D[ind] = D[ind][1469:]\n",
    "\n",
    "ind = sids.index('sid91') # T=657\n",
    "Pobs[ind] = Pobs[ind][602:]\n",
    "Eobs[ind] = Eobs[ind][602:]\n",
    "D[ind] = D[ind][602:]\n",
    "\n",
    "ind = sids.index('sid30') # T=453\n",
    "Pobs[ind] = Pobs[ind][395:]\n",
    "Eobs[ind] = Eobs[ind][395:]\n",
    "D[ind] = D[ind][395:]\n",
    "\n",
    "ind = sids.index('sid1966') # T=334\n",
    "Pobs[ind] = Pobs[ind][300:]\n",
    "Eobs[ind] = Eobs[ind][300:]\n",
    "D[ind] = D[ind][300:]\n",
    "\n",
    "ind = sids.index('sid395') # T=247\n",
    "Pobs[ind] = Pobs[ind][196:]\n",
    "Eobs[ind] = Eobs[ind][196:]\n",
    "D[ind] = D[ind][196:]\n",
    "\n",
    "ind = sids.index('sid1025') # T=245\n",
    "Pobs[ind] = Pobs[ind][178:]\n",
    "Eobs[ind] = Eobs[ind][178:]\n",
    "D[ind] = D[ind][178:]\n",
    "\n",
    "ind = sids.index('sid36')\n",
    "Pobs[ind] = Pobs[ind][:48]\n",
    "Eobs[ind] = Eobs[ind][:48]\n",
    "D[ind] = D[ind][:48]\n",
    "\n",
    "ind = sids.index('sid801')\n",
    "Pobs[ind] = Pobs[ind][33:]\n",
    "Eobs[ind] = Eobs[ind][33:]\n",
    "D[ind] = D[ind][33:]\n",
    "\n",
    "ind = sids.index('sid960')\n",
    "Pobs[ind] = Pobs[ind][72:]\n",
    "Eobs[ind] = Eobs[ind][72:]\n",
    "D[ind] = D[ind][72:]\n",
    "\n",
    "ind = sids.index('sid1006')\n",
    "Pobs[ind] = Pobs[ind][47:]\n",
    "Eobs[ind] = Eobs[ind][47:]\n",
    "D[ind] = D[ind][47:]\n",
    "\n",
    "ind = sids.index('sid1022')\n",
    "Pobs[ind] = Pobs[ind][:39]\n",
    "Eobs[ind] = Eobs[ind][:39]\n",
    "D[ind] = D[ind][:39]\n",
    "\n",
    "ind = sids.index('sid456')\n",
    "Pobs[ind] = Pobs[ind][99:137]\n",
    "Eobs[ind] = Eobs[ind][99:137]\n",
    "D[ind] = D[ind][99:137]\n",
    "\n",
    "ind = sids.index('sid965')\n",
    "Pobs[ind] = Pobs[ind][88:]\n",
    "Eobs[ind] = Eobs[ind][88:]\n",
    "D[ind] = D[ind][88:]\n",
    "\n",
    "ind = sids.index('sid915')\n",
    "Pobs[ind] = Pobs[ind][93:]\n",
    "Eobs[ind] = Eobs[ind][93:]\n",
    "D[ind] = D[ind][93:]\n",
    "\n",
    "print(sorted([len(x) for x in D]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove flat drug at the beginning or end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 23, 23, 25, 28, 29, 29, 31, 31, 32, 33, 34, 35, 37, 37, 38, 40, 45, 45, 45, 46, 48, 48, 48, 48, 48, 48, 49, 49, 49, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 52, 52, 53, 53, 54, 54, 55, 55, 55, 56, 56, 57, 57, 57, 59, 59, 59, 60, 60, 60, 60, 60, 61, 63, 65, 65, 65, 66, 66, 66, 67, 67, 68, 68, 68, 70, 72, 77, 78, 80, 98, 102]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sids)):\n",
    "    d = D[i].sum(axis=1)\n",
    "    \n",
    "    start = 0\n",
    "    for gi, g in enumerate(groupby(d)):\n",
    "        if gi==0:\n",
    "            j, k = g\n",
    "            ll = len(list(k))\n",
    "            if j==0:\n",
    "                start = ll\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    end = 0\n",
    "    for gi, g in enumerate(groupby(d[::-1])):\n",
    "        if gi==0:\n",
    "            j, k = g\n",
    "            ll = len(list(k))\n",
    "            if j==0:\n",
    "                end = ll\n",
    "        else:\n",
    "            break\n",
    "    end = len(d)-end\n",
    "    \n",
    "    Pobs[i] = Pobs[i][start:end]\n",
    "    Eobs[i] = Eobs[i][start:end]\n",
    "    D[i] = D[i][start:end]\n",
    "    \n",
    "print(sorted([len(x) for x in D]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make sure the first T0 points are not NaN to initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 20, 23, 24, 25, 28, 29, 29, 31, 31, 32, 33, 34, 35, 37, 37, 38, 44, 45, 45, 45, 46, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 50, 50, 50, 50, 50, 51, 51, 51, 51, 52, 52, 53, 53, 54, 54, 54, 55, 55, 55, 56, 56, 57, 57, 57, 58, 59, 59, 60, 60, 60, 60, 60, 64, 65, 65, 65, 66, 66, 66, 67, 67, 67, 68, 68, 69, 72, 73, 78, 80, 102]\n"
     ]
    }
   ],
   "source": [
    "T0 = 2\n",
    "for i in range(len(sids)):\n",
    "    # move along time to find the first time when 2 points are not NaN\n",
    "    found = False\n",
    "    for t in range(len(D[i])-T0):\n",
    "        if all([not np.isnan(Pobs[i][t+j]) for j in range(T0)]):\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        print(sids[i], 'not found first T0 points being not NaN.')\n",
    "        continue\n",
    "    Eobs[i] = Eobs[i][t:]\n",
    "    Pobs[i] = Pobs[i][t:]\n",
    "    D[i] = D[i][t:]\n",
    "    \n",
    "print(sorted([len(x) for x in D]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pad to same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 102)\n",
      "(82, 102)\n",
      "(82, 102, 8)\n",
      "(82, 1)\n"
     ]
    }
   ],
   "source": [
    "Ts = np.array([len(x) for x in D])\n",
    "maxT = np.max(Ts)\n",
    "\n",
    "for i in range(len(sids)):\n",
    "    Eobs[i] = np.r_[Eobs[i], np.zeros(maxT-len(Eobs[i]), dtype=int)-1]\n",
    "    Pobs[i] = np.r_[Pobs[i], np.zeros(maxT-len(Pobs[i]))+np.nan]\n",
    "    D[i] = np.r_[D[i], np.zeros((maxT-len(D[i]), D[i].shape[1]))]\n",
    "\n",
    "Eobs = np.array(Eobs)\n",
    "Pobs = np.array(Pobs)\n",
    "D = np.array(D)\n",
    "N = len(D)\n",
    "T = D.shape[1]\n",
    "Ts = np.array([np.sum(~np.isnan(x)) for x in Pobs])\n",
    "\n",
    "print(Eobs.shape)\n",
    "print(Pobs.shape)\n",
    "print(D.shape)\n",
    "print(C.shape)\n",
    "\n",
    "#sio.savemat('CDE.mat', {'C':C, 'D':D, 'E':Pobs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from hashlib import md5\n",
    "\n",
    "def StanModel_cache(model_code, model_name=None, **kwargs):\n",
    "    \"\"\"Use just as you would `stan`\"\"\"\n",
    "    code_hash = md5(model_code.encode('ascii')).hexdigest()\n",
    "    if model_name is None:\n",
    "        cache_fn = 'cached-model-{}.pkl'.format(code_hash)\n",
    "    else:\n",
    "        cache_fn = 'cached-{}-{}.pkl'.format(model_name, code_hash)\n",
    "    try:\n",
    "        sm = pickle.load(open(cache_fn, 'rb'))\n",
    "    except:\n",
    "        sm = pystan.StanModel(model_code=model_code)\n",
    "        with open(cache_fn, 'wb') as f:\n",
    "            pickle.dump(sm, f)\n",
    "    else:\n",
    "        print(\"Using cached StanModel\")\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_424944210e279abf01d99d5299cf3503 NOW.\n"
     ]
    }
   ],
   "source": [
    "with open('model_hierarchical.stan', 'r') as f:\n",
    "    pd_model = f.read()\n",
    "model = StanModel_cache(model_code=pd_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_state = 2020\n",
    "\n",
    "# leave one patient out cross-validation\n",
    "fit_res = []\n",
    "Ep_tr_sim = []\n",
    "Ep_te_sim = []\n",
    "Ep_te_baseline = []\n",
    "sids_te = []\n",
    "for si, sid in enumerate(sids):\n",
    "    print('\\n############### [%d/%d] %s ###############\\n'%(si+1, len(sids), sid))\n",
    "    sids_te.append(sid)\n",
    "    \n",
    "    trids = [x for x in range(len(sids)) if sids[x]!=sid]\n",
    "    Eobstr = Eobs[trids]\n",
    "    Pobstr = Pobs[trids]\n",
    "    Dtr = D[trids]\n",
    "    Ctr = C[trids]\n",
    "    Ttr = Ts[trids]\n",
    "    \n",
    "    teid = [si]\n",
    "    Eobste = np.array(Eobs[teid])\n",
    "    Pobste = np.array(Pobs[teid])\n",
    "    Dte = np.array(D[teid])\n",
    "    Cte = np.array(C[teid])\n",
    "    #Tte\n",
    "    \n",
    "    # standardize features\n",
    "    Cmean = Ctr.mean(axis=0)\n",
    "    Cstd = Ctr.std(axis=0)\n",
    "    Ctr = (Ctr-Cmean)/Cstd\n",
    "    Cte = (Cte-Cmean)/Cstd\n",
    "    #Ctr = Ctr/100\n",
    "    #Cte = Cte/100\n",
    "    Dtr_nan = np.array(Dtr)\n",
    "    Dtr_nan[Dtr_nan==0] = np.nan\n",
    "    Dmax  = np.nanpercentile(Dtr_nan, 95, axis=(0,1))\n",
    "    Dtr = Dtr/Dmax\n",
    "    Dte = Dte/Dmax\n",
    "    \n",
    "    # combine the first Tcomb steps of testing to training\n",
    "    # to enable patient-specific training\n",
    "    Eobstr2 = np.r_[Eobstr, Eobste]\n",
    "    Pobstr2 = np.r_[Pobstr, Pobste]\n",
    "    Dtr2 = np.r_[Dtr, Dte]\n",
    "    Ctr2 = np.r_[Ctr, Cte]\n",
    "    Tcomb = 10  # must be >T0\n",
    "    Eobste[:,Tcomb:] = -1\n",
    "    Pobste[:,Tcomb:] = np.nan\n",
    "    Dte[:,Tcomb:] = 0\n",
    "    Ttr = np.r_[Ttr, [np.sum(~np.isnan(Pobste[ii])) for ii in range(len(Pobste))]]\n",
    "    Eobstr = np.r_[Eobstr, Eobste]\n",
    "    Pobstr = np.r_[Pobstr, Pobste]\n",
    "    Dtr = np.r_[Dtr, Dte]\n",
    "    Ctr = np.r_[Ctr, Cte]\n",
    "    \n",
    "    Eobstr_flatten = Eobstr[:,T0:].flatten()#\n",
    "    not_empty_ids = np.where(Eobstr_flatten!=-1)[0]\n",
    "    not_empty_num = len(not_empty_ids)\n",
    "    Eobstr_flatten_nonan = Eobstr_flatten[not_empty_ids]\n",
    "    \n",
    "    # generate sample weights that balances different lengths\n",
    "    sample_weights = np.zeros_like(Eobstr[:,T0:]) + 1/(Ttr-T0).reshape(-1,1)#\n",
    "    sample_weights = sample_weights.flatten()[not_empty_ids]\n",
    "    sample_weights = sample_weights/sample_weights.mean()\n",
    "    \n",
    "    data_feed = {'W':W,\n",
    "                 'N':N,\n",
    "                 'T':T,\n",
    "                 'T0':T0,\n",
    "                 'ND':Dtr.shape[-1],\n",
    "                 #'NC':Ctr.shape[-1],\n",
    "                 'not_empty_num':not_empty_num,\n",
    "                 'not_empty_ids':not_empty_ids+1,  # +1 for stan\n",
    "                 'sample_weights':sample_weights,\n",
    "                 'Eobs_flatten_nonan':Eobstr_flatten_nonan,\n",
    "                 'D':Dtr.transpose(1,0,2),  # because matrix[N,ND] D[T];\n",
    "                 #'C':Ctr,\n",
    "                 #'p_start':Pobstr[:,:T0],\n",
    "                 'A_start':logit(Pobstr[:,:T0]),\n",
    "                 }\n",
    "    model = StanModel_cache(model_code=pd_model)\n",
    "    \n",
    "    # sampling\n",
    "    \n",
    "    # try multiple times with few iterations\n",
    "    # the fastest one is usually the one that converges\n",
    "    times = []\n",
    "    for rs in range(3):\n",
    "        print('Try %d'%(rs+1,))\n",
    "        st = timeit.default_timer()\n",
    "        fit = model.sampling(data=data_feed, iter=100, verbose=True, chains=1, seed=random_state+rs)\n",
    "        et = timeit.default_timer()\n",
    "        times.append(et-st)\n",
    "        \n",
    "    # sample many iterations for the one that can converge\n",
    "    fit = model.sampling(data=data_feed, iter=1000, verbose=True, chains=1, seed=random_state+np.argmin(times))\n",
    "                      #control={'max_treedepth':9})#, 'adapt_delta':0.9})\n",
    "    print(fit.stansummary(pars=['sigma_a0','sigma_a1','sigma_a2','sigma_b']))\n",
    "    \n",
    "    df = fit.to_dataframe(pars=['a0','a1','a2','b','sigma_a0','sigma_a1','sigma_a2','sigma_b'])\n",
    "    \n",
    "    # save\n",
    "    df.to_csv('fit_dataframe_%s.csv'%sid, index=False)\n",
    "    with open('model_fit_%s.pkl'%sid, 'wb') as f:\n",
    "        pickle.dump([model, fit], f)\n",
    "    \"\"\"\n",
    "    with open('model_fit_%s.pkl'%sid, 'rb') as f:\n",
    "        model, fit = pickle.load(f)\n",
    "    df = pd.read_csv('fit_dataframe_%s.csv'%sid)\n",
    "    \"\"\"\n",
    "    fit_res.append(fit)\n",
    "    \n",
    "    # predict\n",
    "    Epte = []\n",
    "    Ppte = []\n",
    "    start = 0#len(df)//2\n",
    "    for i in tqdm(range(start,len(df))):\n",
    "        a0 = np.array([df['a0[%d]'%ii].iloc[i] for ii in range(1,len(Dtr)+1)])\n",
    "        a1 = np.array([df['a1[%d]'%ii].iloc[i] for ii in range(1,len(Dtr)+1)])\n",
    "        a2 = np.array([df['a2[%d]'%ii].iloc[i] for ii in range(1,len(Dtr)+1)])\n",
    "        b = np.array([[df['b[%d,%d]'%(jj,ii)].iloc[i] for ii in range(1,Dtr.shape[-1]+1)] for jj in range(1,len(Dtr)+1)])\n",
    "        \n",
    "        A = np.zeros((N,T))+np.nan\n",
    "        for t in range(T0):\n",
    "            A[:,t] = logit(Pobstr2[:,t])\n",
    "        for t in range(T0, T):\n",
    "            A[:,t] = a0 + a1*A[:,t-1] + a2*A[:,t-2] - np.sum(Dtr2[:,t]*b, axis=1)\n",
    "        p = sigmoid(A)\n",
    "        \n",
    "        Ppte.append(p)\n",
    "        Epte.append(np.random.binomial(W, p))\n",
    "    Ppte = np.array(Ppte)\n",
    "    Epte = np.array(Epte)\n",
    "    \n",
    "    Epte = Epte/W\n",
    "    Ep_tr_sim.append(Epte[:,:-1])\n",
    "    Ep_te_sim.append(Epte[:,-1])\n",
    "    \n",
    "    # baseline\n",
    "    \n",
    "    # first decide which value to carry forward\n",
    "    # it should be the (Tcomb-1)-th, but it can be NaN, search backwards until non-NaN\n",
    "    for tt in range(Tcomb-1,-1,-1):\n",
    "        if not np.isnan(Pobstr[-1][tt]):\n",
    "            break\n",
    "    Epte_baseline = np.random.binomial(W, Pobstr[-1][tt], size=len(Ppte))\n",
    "    Epte_baseline = Epte_baseline/W\n",
    "    Epte_baseline = np.array([Epte_baseline]*(T-tt)).T\n",
    "    Epte_baseline = np.c_[np.array([Pobstr[-1][:tt]]*len(Ppte)), Epte_baseline]\n",
    "        \n",
    "    Ep_te_baseline.append(Epte_baseline)\n",
    "    \n",
    "    plt.close()\n",
    "    fig = plt.figure()\n",
    "    ax1=fig.add_subplot(211)\n",
    "    Epte3 = np.percentile(Epte, (50,2.5,97.5), axis=0)\n",
    "    ax1.plot(Epte3[:,-1,:].T,c='r')\n",
    "    ax1.plot(Pobstr2[-1],c='k')\n",
    "    ax1.axvline(Tcomb, c='r', ls='--', lw=2)\n",
    "    ax2=fig.add_subplot(212)\n",
    "    ax2.plot(Dtr2[-1]*Dmax)\n",
    "    ax2.axvline(Tcomb, c='r', ls='--', lw=2)\n",
    "    #plt.show()\n",
    "    plt.savefig('%s.png'%sid)\n",
    "    \n",
    "    with open('results.pickle', 'wb') as ff:\n",
    "        pickle.dump({'fit_res':fit_res,\n",
    "                     #'Ep_tr_sim':Ep_tr_sim,\n",
    "                     'Ep_te_sim':Ep_te_sim,\n",
    "                     'Ep_te_baseline':Ep_te_baseline,\n",
    "                     'E_te':Pobstr2[-1],\n",
    "                     'D_te':Dtr2[-1]*Dmax,\n",
    "                     'sids':sids_te}, ff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
